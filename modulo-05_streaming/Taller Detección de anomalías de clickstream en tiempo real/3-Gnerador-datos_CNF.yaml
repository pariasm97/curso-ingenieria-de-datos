AWSTemplateFormatVersion: 2010-09-09
Description: Kinesis Analytics Lab - S3 Ingestion (Smart Router & Fixed TLS)

Parameters:
  FlinkVersion:
    Type: String
    Default: 1.15.4
  GlueDatabaseName:
    Type: String
    Default: prelab_kda_db

Resources:
  # ---------------------------------------------------------
  # 1. S3 BUCKET & LAMBDA INGESTOR
  # ---------------------------------------------------------
  IngestionBucket:
    Type: AWS::S3::Bucket
    DependsOn: IngestLambdaPermission
    Properties:
      NotificationConfiguration:
        LambdaConfigurations:
          - Event: 's3:ObjectCreated:*'
            Function: !GetAtt CSVToKinesisRouter.Arn
            Filter:
              S3Key:
                Rules:
                  - Name: suffix
                    Value: .csv

  IngestLambdaRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal: { Service: lambda.amazonaws.com }
            Action: sts:AssumeRole
      Policies:
        - PolicyName: S3KinesisAccess
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Allow
                Action: ['logs:*', 's3:GetObject']
                Resource: '*'
              - Effect: Allow
                Action: ['kinesis:PutRecord', 'kinesis:PutRecords']
                Resource: 
                  - !GetAtt ClickStream.Arn
                  - !GetAtt ImpressionStream.Arn
                  - !GetAtt TickerStream.Arn

  CSVToKinesisRouter:
    Type: AWS::Lambda::Function
    Properties:
      Handler: index.lambda_handler
      Role: !GetAtt IngestLambdaRole.Arn
      Runtime: python3.9
      Timeout: 300
      Environment:
        Variables:
          CLICK_STREAM: !Ref ClickStream
          IMPRESSION_STREAM: !Ref ImpressionStream
          TICKER_STREAM: !Ref TickerStream
      Code:
        ZipFile: |
          import boto3
          import csv
          import json
          import os
          import time

          s3 = boto3.client('s3')
          kinesis = boto3.client('kinesis')

          def lambda_handler(event, context):
              for record in event['Records']:
                  bucket = record['s3']['bucket']['name']
                  key = record['s3']['object']['key']
                  
                  # 1. Determinar el Stream destino basado en el nombre del archivo
                  stream_name = None
                  if 'click' in key.lower():
                      stream_name = os.environ['CLICK_STREAM']
                  elif 'impression' in key.lower():
                      stream_name = os.environ['IMPRESSION_STREAM']
                  elif 'ticker' in key.lower():
                      stream_name = os.environ['TICKER_STREAM']
                  
                  if not stream_name:
                      print(f"SKIPPING: Could not determine stream for file {key}")
                      continue

                  print(f"PROCESSING: {key} -> {stream_name}")
                  
                  # 2. Leer y enviar
                  try:
                      obj = s3.get_object(Bucket=bucket, Key=key)
                      lines = obj['Body'].read().decode('utf-8').splitlines()
                      reader = csv.DictReader(lines)
                      
                      count = 0
                      for row in reader:
                          # Convertir CSV row -> JSON (Esto simula al Kinesis Data Generator)
                          partition_key = row.get('userid', str(time.time()))
                          
                          kinesis.put_record(
                              StreamName=stream_name,
                              Data=json.dumps(row),
                              PartitionKey=partition_key
                          )
                          count += 1
                          # Pequeño delay para simular tráfico real (opcional)
                          # time.sleep(0.01) 
                          
                      print(f"SUCCESS: Sent {count} records to {stream_name}")
                      
                  except Exception as e:
                      print(f"ERROR: {str(e)}")
                      raise e
              return {'status': 'ok'}

  IngestLambdaPermission:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !GetAtt CSVToKinesisRouter.Arn
      Action: lambda:InvokeFunction
      Principal: s3.amazonaws.com
      SourceAccount: !Ref AWS::AccountId

  # ---------------------------------------------------------
  # 2. CORE INFRASTRUCTURE (TLS Fix & Dynamic Names)
  # ---------------------------------------------------------
  ArtifactBucket:
    Type: AWS::S3::Bucket
    Properties:
      VersioningConfiguration: { Status: Enabled }

  # Streams with Dynamic Names to avoid "Already Exists"
  TickerStream:
    Type: 'AWS::Kinesis::Stream'
    Properties: { Name: !Sub '${AWS::StackName}-tickerstream', ShardCount: 1 }
  ClickStream:
    Type: 'AWS::Kinesis::Stream'
    Properties: { Name: !Sub '${AWS::StackName}-clickstream', ShardCount: 1 }
  ImpressionStream:
    Type: 'AWS::Kinesis::Stream'
    Properties: { Name: !Sub '${AWS::StackName}-impressionstream', ShardCount: 1 }
  CtrStream:
    Type: 'AWS::Kinesis::Stream'
    Properties: { Name: !Sub '${AWS::StackName}-ctrstream', ShardCount: 1 }
  DestinationStream:
    Type: 'AWS::Kinesis::Stream'
    Properties: { Name: !Sub '${AWS::StackName}-destinationstream', ShardCount: 1 }
  AnomalyDetectionStream:
    Type: 'AWS::Kinesis::Stream'
    Properties: { Name: !Sub '${AWS::StackName}-anomalydetectionstream', ShardCount: 1 }

  # ... (Rest of roles and standard resources maintained for brevity) ...
  # ... (Roles: LambdaExecutionRole, ServiceExecutionRole, KinesisAnalyticsServiceExecutionRole, etc.) ...
  
  # IMPORTANT: CodeBuild with Fix for TLS Handshake
  RcfExampleBuildProject:
    Type: AWS::CodeBuild::Project
    Properties:
      ServiceRole: !GetAtt CodeBuildServiceRole.Arn
      Artifacts: { Type: CODEPIPELINE }
      Environment:
        Type: LINUX_CONTAINER
        ComputeType: BUILD_GENERAL1_LARGE
        Image: aws/codebuild/amazonlinux2-x86_64-standard:5.0 # FIXED IMAGE
      Source:
        Type: CODEPIPELINE
        BuildSpec: !Sub |
          version: 0.2
          phases:
            install:
              runtime-versions:
                java: corretto11
            build:
              commands:
                - 'cd data-engineering-for-aws-immersion-day-* || :'
                - 'cd AnomalyDetection/RandomCutForest || :'
                - mvn clean package --B -Dflink.version=${FlinkVersion}
          artifacts:
            files:
              - target/flink-random-cut-forest-example-*.jar
              - data-engineering-for-aws-immersion-day-*/AnomalyDetection/RandomCutForest/target/flink-random-cut-forest-example-*.jar
            discard-paths: yes
      TimeoutInMinutes: 15

  # ... (Pipelines and other standard resources) ...
  
  # Roles needed for the build (Condensed for copy-paste)
  LambdaExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement: [{ Effect: Allow, Principal: { Service: lambda.amazonaws.com }, Action: sts:AssumeRole }]
      Policies:
        - PolicyName: root
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - { Effect: Allow, Action: ['logs:*', 's3:*', 'kinesis:*', 'glue:*', 'sns:*', 'codepipeline:*'], Resource: '*' }

  CodePipelineServiceRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement: [{ Effect: Allow, Principal: { Service: codepipeline.amazonaws.com }, Action: sts:AssumeRole }]
      Policies:
        - PolicyName: root
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - { Effect: Allow, Action: ['s3:*', 'codebuild:*', 'lambda:*'], Resource: '*' }

  CodeBuildServiceRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement: [{ Effect: Allow, Principal: { Service: codebuild.amazonaws.com }, Action: sts:AssumeRole }]
      Policies:
        - PolicyName: root
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - { Effect: Allow, Action: ['logs:*', 's3:*'], Resource: '*' }

  # Pipeline Definition
  BuildPipeline:
    Type: AWS::CodePipeline::Pipeline
    Properties:
      RoleArn: !GetAtt CodePipelineServiceRole.Arn
      Stages:
        - Name: Source
          Actions:
            - Name: RcfExampleSourceAction
              ActionTypeId: { Category: Source, Owner: AWS, Version: "1", Provider: S3 }
              OutputArtifacts: [{ Name: RcfExampleSource }]
              Configuration:
                S3Bucket: !Ref ArtifactBucket
                S3ObjectKey: sources/amazon-kinesis-analytics-rcf-example.zip
              RunOrder: 1
        - Name: BuildRcfExample
          Actions: 
            - Name: BuildRcfExample
              InputArtifacts: [{ Name: RcfExampleSource }]
              OutputArtifacts: [{ Name: BuildRcfExampleOutput }]
              ActionTypeId: { Category: Build, Owner: AWS, Version: "1", Provider: CodeBuild }
              Configuration:
                ProjectName: !Ref RcfExampleBuildProject
                PrimarySource: RcfExampleSource
              RunOrder: 1
        - Name: Copy
          Actions:
            - Name: CopyRcfExample
              InputArtifacts: [{ Name: BuildRcfExampleOutput }]
              ActionTypeId: { Category: Deploy, Owner: AWS, Version: "1", Provider: S3 }
              Configuration: { BucketName: !Ref ArtifactBucket, Extract: true }
              RunOrder: 1
            - Name: NotifyCloudformation
              ActionTypeId: { Category: Invoke, Owner: AWS, Version: "1", Provider: Lambda }
              Configuration: { FunctionName: !Ref NotifyWaitConditionLambdaFunction }
              RunOrder: 2
      ArtifactStore: { Type: S3, Location: !Ref ArtifactBucket }

  BuildCompleteWaitCondition:
    Type: AWS::CloudFormation::WaitCondition
    Properties: { Count: 1, Handle: !Ref BuildCompleteWaitHandle, Timeout: "1800" }
  BuildCompleteWaitHandle:
    Type: AWS::CloudFormation::WaitConditionHandle

  NotifyWaitConditionLambdaFunction:
    Type: AWS::Lambda::Function
    Properties:
      Handler: index.handler
      Role: !GetAtt LambdaExecutionRole.Arn
      Runtime: python3.9
      Timeout: 10
      Code:
        ZipFile: !Sub |
          import json
          import boto3
          import urllib.request
          code_pipeline = boto3.client('codepipeline')
          def handler(event, context):
            job_id = event['CodePipeline.job']['id']
            url = '${BuildCompleteWaitHandle}'
            headers = { "Content-Type": "" }
            data = { "Status": "SUCCESS", "Reason": "Done", "UniqueId": "Rcf", "Data": "Done" }
            try:
              req = urllib.request.Request(url, headers=headers, data=bytes(json.dumps(data), encoding="utf-8"), method='PUT')
              urllib.request.urlopen(req)
              code_pipeline.put_job_success_result(jobId=job_id)
            except Exception as e:
              code_pipeline.put_job_failure_result(jobId=job_id, failureDetails={'message': str(e), 'type': 'JobFailed'})

  DownloadSources:
    Type: Custom::DownloadSources
    Properties: { ServiceToken: !GetAtt DownloadSourcesFunction.Arn }

  DownloadSourcesFunction:
    Type: AWS::Lambda::Function
    Properties:
      Handler: index.handler
      Role: !GetAtt LambdaExecutionRole.Arn
      MemorySize: 256
      Timeout: 300
      Code:
        ZipFile: !Sub |
          import boto3
          import cfnresponse
          from urllib.request import urlopen
          def handler(event, context):
            if event['RequestType'] != 'Delete':
              s3 = boto3.client('s3')
              url = 'https://github.com/${GitOrganizationName}/data-engineering-for-aws-immersion-day/archive/${Release}.zip'
              try:
                  src = urlopen(url)
                  s3.put_object(Bucket='${ArtifactBucket}',Key='sources/amazon-kinesis-analytics-rcf-example.zip',Body=src.read())
                  cfnresponse.send(event, context, cfnresponse.SUCCESS, {})
              except:
                  cfnresponse.send(event, context, cfnresponse.FAILED, {})
            else:
                cfnresponse.send(event, context, cfnresponse.SUCCESS, {})

  # ... (GlueDatabase, AppNotebook, etc. maintained) ...
  GlueDatabase:
    Type: AWS::Glue::Database
    Properties:
      CatalogId: !Ref AWS::AccountId
      DatabaseInput: { Name: !Ref GlueDatabaseName }

  RealtimeApplicationNotebook:
    Type: AWS::KinesisAnalyticsV2::Application
    Properties:
      ApplicationMode: INTERACTIVE
      ApplicationName: !Sub '${AWS::StackName}-RealtimeApplicationNotebook'
      RuntimeEnvironment: ZEPPELIN-FLINK-3_0
      ServiceExecutionRole: !GetAtt KinesisAnalyticsServiceExecutionRole.Arn
      ApplicationConfiguration:
        FlinkApplicationConfiguration: { ParallelismConfiguration: { Parallelism: 2, ConfigurationType: CUSTOM } }
        ZeppelinApplicationConfiguration:
          CatalogConfiguration: { GlueDataCatalogConfiguration: { DatabaseARN: !Sub "arn:aws:glue:${AWS::Region}:${AWS::AccountId}:database/${GlueDatabase}" } }
          CustomArtifactsConfiguration:
            - { ArtifactType: DEPENDENCY_JAR, MavenReference: { GroupId: org.apache.flink, ArtifactId: flink-sql-connector-kinesis, Version: 1.15.4 } }
            - { ArtifactType: DEPENDENCY_JAR, MavenReference: { GroupId: org.apache.flink, ArtifactId: flink-connector-kafka, Version: 1.15.4 } }
            - { ArtifactType: DEPENDENCY_JAR, MavenReference: { GroupId: software.amazon.msk, ArtifactId: aws-msk-iam-auth, Version: 1.1.6 } }
            - { ArtifactType: "UDF", S3ContentLocation: { BucketARN: !Sub 'arn:aws:s3:::${ArtifactBucket}', FileKey: 'flink-random-cut-forest-example-1.0.jar' } }
    DependsOn: BuildCompleteWaitCondition

  KinesisAnalyticsServiceExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement: [{ Effect: Allow, Principal: { Service: kinesisanalytics.amazonaws.com }, Action: 'sts:AssumeRole' }]
      Policies:
        - PolicyName: glue-kinesis-access
          PolicyDocument:
            Version: 2012-10-17
            Statement: [{ Effect: Allow, Action: ['glue:*', 'kinesis:*', 'kinesisanalytics:*', 's3:*'], Resource: '*' }]

  # Consumer Lambda (Optional in Lab but included in original CFN)
  RCFBeconAnomalyResponse:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub '${AWS::StackName}-AnomalyResponse'
      Handler: index.handler
      MemorySize: 256
      Role: !GetAtt LambdaExecutionRole.Arn
      Runtime: nodejs16.x
      Code:
        ZipFile: |
          exports.handler = function(event, context) {
            event.Records.forEach(function(record) {
              var payload = new Buffer(record.kinesis.data, 'base64').toString('ascii');
              console.log("Anomaly Detected:", payload);
            });
          };

  InboundStreamLambdaFunctionEventSourceMapping:
    Type: AWS::Lambda::EventSourceMapping
    Properties: 
      BatchSize: 100 
      Enabled: true
      EventSourceArn: !GetAtt AnomalyDetectionStream.Arn
      FunctionName: !GetAtt RCFBeconAnomalyResponse.Arn
      StartingPosition: TRIM_HORIZON

Outputs:
  IngestionBucketName:
    Description: Upload clicks.csv here
    Value: !Ref IngestionBucket