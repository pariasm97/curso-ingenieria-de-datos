#  Lab: Detecci贸n de Anomal铆as en Tiempo Real en Clickstream con Amazon Managed Service for Apache Flink

Este laboratorio es parte del taller **Amazon Managed Service for Apache Flink Workshop**. El objetivo es construir una canalizaci贸n (pipeline) de procesamiento de streaming para detectar anomal铆as en datos de clickstream (comportamiento de usuarios) utilizando el algoritmo **Random Cut Forest (RCF)** sobre Apache Flink.

## Tabla de Contenidos
- [Descripci贸n del Escenario](#descripci贸n-del-escenario)
- [Arquitectura](#arquitectura)
- [Requisitos Previos](#requisitos-previos)
- [Instrucciones Paso a Paso](#instrucciones-paso-a-paso)
  - [1. Configuraci贸n de Infraestructura](#1-configuraci贸n-de-infraestructura)
  - [2. Generaci贸n de Datos (Data Producer)](#2-generaci贸n-de-datos-data-producer)
  - [3. Despliegue de la Aplicaci贸n Flink](#3-despliegue-de-la-aplicaci贸n-flink)
  - [4. Ejecuci贸n y Monitoreo](#4-ejecuci贸n-y-monitoreo)
  - [5. Verificaci贸n de Resultados](#5-verificaci贸n-de-resultados)
- [Limpieza de Recursos](#limpieza-de-recursos)
- [Referencias](#referencias)

---

## Descripci贸n del Escenario

Los sitios web modernos generan millones de eventos de "clickstream" (vistas de p谩gina, clics en botones, errores, etc.). Detectar anomal铆as en estos flujos en tiempo real (por ejemplo, un aumento repentino de errores 404 o una ca铆da dr谩stica en las compras) es cr铆tico para la operatividad del negocio.

En este laboratorio utilizaremos **Amazon Managed Service for Apache Flink** para procesar estos datos y aplicar el algoritmo **Random Cut Forest (RCF)**. RCF es un algoritmo no supervisado dise帽ado para detectar puntos de datos que divergen del patr贸n normal sin necesidad de etiquetado previo.

---

## Arquitectura

El flujo de datos propuesto es el siguiente:

1.  **Fuente (Source):** Un script en Python genera eventos de clickstream simulados y los inyecta en un **Amazon Kinesis Data Stream** (`InputStream`).
2.  **Procesamiento:** La aplicaci贸n de **Apache Flink** consume el stream, agrega los datos y calcula un `ANOMALY_SCORE`.
3.  **Destino (Sink):** Los resultados (incluyendo el score de anomal铆a) se env铆an a otro **Kinesis Data Stream** (`OutputStream`) para su consumo posterior (ej. Lambda, OpenSearch o Dashboard).

```mermaid
graph LR
    A[Generador de Datos<br>(Python Script)] -->|PutRecord| B[Kinesis Data Stream<br>(Input)]
    B -->|Consumer| C[Amazon Managed Service<br>for Apache Flink]
    C -->|RCF Algorithm| D[Kinesis Data Stream<br>(Output)]
    D -->|Consumo| E[Lambda / OpenSearch]
```
##  Requisitos Previos
Antes de comenzar, aseg煤rate de tener:

Cuenta de AWS: Acceso a la consola de AWS con permisos de Administrador o PowerUser.

Entorno de Desarrollo: AWS Cloud9 (recomendado) o AWS CLI configurado en local.

Java & Maven: Necesario si vas a compilar el c贸digo fuente de la aplicaci贸n Flink.

Python 3.x: Para ejecutar los scripts de generaci贸n de datos.

##  Instrucciones Paso a Paso
1. Configuraci贸n de Infraestructura
Si el taller no provee una plantilla de CloudFormation, crea los recursos manualmente:

Ve a la consola de Amazon Kinesis.

Crea dos Kinesis Data Streams:

ClickstreamInput (Source)

ClickstreamOutput (Sink)

Crea un S3 Bucket (ej. flink-app-artifacts-<tu-nombre>) para almacenar el c贸digo compilado (JAR).

##  2. Generaci贸n de Datos (Data Producer)
Utiliza el script producer.py provisto en los materiales del taller para simular tr谩fico.

Instala la librer铆a boto3 si no la tienes:

Bash
pip install boto3
Ejecuta el productor apuntando al stream de entrada:

Bash
python producer.py --stream ClickstreamInput --region us-east-1
Nota: Mant茅n esta terminal abierta enviando datos durante todo el laboratorio.

3. Despliegue de la Aplicaci贸n Flink
Compilaci贸n: Navega al directorio del c贸digo Java (java-getting-started) y compila el proyecto:

Bash
mvn clean package
Carga: Sube el archivo .jar resultante (ubicado en la carpeta /target) a tu bucket de S3.

Creaci贸n de la App:

Ve a la consola de Amazon Managed Service for Apache Flink.

Selecciona Create streaming application.

Elige Apache Flink como motor.

En Code location, selecciona el bucket y el objeto .jar que acabas de subir.

4. Ejecuci贸n y Configuraci贸n
Configura las propiedades de ejecuci贸n (Runtime Properties) para conectar los streams:

En la configuraci贸n de la aplicaci贸n, a帽ade un Group ID: FlinkAppProperties.

A帽ade los pares Key-Value:

InputStreamName: ClickstreamInput

OutputStreamName: ClickstreamOutput

Region: us-east-1

Aseg煤rate de que el Rol de IAM asociado tenga permisos de lectura/escritura en Kinesis y acceso al bucket S3.

Presiona Run para iniciar la aplicaci贸n.

5. Verificaci贸n de Resultados
Una vez que la aplicaci贸n pase al estado Running:

Revisa la pesta帽a Monitoring en la consola de Flink para verificar que hay m茅tricas de BytesReceived y RecordsWritten.

Para inspeccionar los datos de salida, ejecuta el script consumidor:

Bash
python consumer.py --stream ClickstreamOutput --region us-east-1
Analiza la salida JSON. Busca el campo anomaly_score:

Valores cercanos a 0 indican tr谩fico normal.

Valores altos indican anomal铆as detectadas por el algoritmo RCF.

Limpieza de Recursos
Para evitar costos innecesarios, elimina los recursos en el siguiente orden al finalizar:

Detener la aplicaci贸n Flink (Stop Application).

Eliminar la aplicaci贸n Flink.

Eliminar los streams de Kinesis (ClickstreamInput, ClickstreamOutput).

Vaciar y eliminar el bucket de S3.

