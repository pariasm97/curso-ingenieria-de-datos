Transform: AWS::SecretsManager-2020-07-23

Description: "Sample Module Team template"

Parameters:
  # Optional parameters passed by the Event Engine to the stack.
  # EEEventId:
  #   Description: "Unique ID of this Event"
  #   Type: String
  # EETeamId:
  #   Description: "Unique ID of this Team"
  #   Type: String
  # EEModuleId:
  #   Description: "Unique ID of this module"
  #   Type: String
  # EEModuleVersion:
  #   Description: "Version of this module"
  #   Type: String
  # EEAssetsBucket:
  #   Description: "Region-specific assets S3 bucket name (e.g. ee-assets-prod-us-east-1)"
  #   Type: String
  # EEAssetsKeyPrefix:
  #   Description: "S3 key prefix where this modules assets are stored. (e.g. modules/my_module/v1/)"
  #   Type: String
  # EECentralAccountId:
  #   Description: "AWS Account Id of the Central account"
  #   Type: String
  # EETeamRoleArn:
  #   Description: "ARN of the Team Role"
  #   Type: String
  # EEKeyPair:
  #   Description: "Name of the EC2 KeyPair generated for the Team"
  #   Type: AWS::EC2::KeyPair::KeyName
  # Your own parameters for the stack. NOTE: All these parameters need to have a default value.
  EEAssetsBucket:
    Description: "Region-specific assets S3 bucket name (e.g. ee-assets-prod-us-east-1)"
    Type: String
    Default: "ee-assets-prod-us-east-1"
  EEAssetsKeyPrefix:
    Description: "S3 key prefix where this modules assets are stored. (e.g. modules/my_module/v1/)"
    Type: String
    Default: "modules/599e7c685a254c2b892cdbf58a7b3b4f/v1/"
  KDADatabaseName:
    Type: String
    MinLength: "1"
    Description: "Database Name for KDA Application."
    Default: "kinesislab"
  KDAKinesisStreamName:
    Type: String
    MinLength: "1"
    Description: "Kinesis stream name for loading data."
    Default: "input-stream"
  CodeEditorUser:
    Type: String
    Description: UserName for Code Editor
    Default: participant
  InstanceName:
    Type: String
    Description: Code Editor EC2 instance name
    Default: CodeEditorInstance
  InstanceVolumeSize:
    Type: Number
    Description: Code Editor EC2 instance volume size in GB
    Default: 40
  InstanceType:
    Description: Code Editor EC2 instance type
    Type: String
    Default: t4g.medium
  InstanceOperatingSystem:
    Description: Code Editor EC2 operating system
    Type: String
    Default: AmazonLinux-2023
    AllowedValues: ['AmazonLinux-2023', 'Ubuntu-22', 'Ubuntu-24']
  HomeFolder:
    Type: String
    Description: Folder to open in Code Editor
    Default: /workshop
  DevServerBasePath:
    Type: String
    Description: Base path for the application to be added to Nginx sites-available list
    Default: ''
  DevServerPort:
    Type: Number
    Description: Port for the DevServer
    Default: 8081
  RepoUrl:
    Description: Remote repo URL to clone. To not clone a remote repo, leave blank
    Type: String
    Default: ''
  AssetZipS3Path:
    Description: S3 path holding the asset zip file to be copied into the home folder. To not include any assets, leave blank
    Type: String
    Default: ''
  BranchZipS3Path:
    Description: S3 path holding the branches zip file to be checked into the git repo, with each folder being a branch. The content of each folder will added as under a branch, with the folder name being used as the branch name. To leave the empty, leave blank
    Type: String
    Default: ''
  FolderZipS3Path:
    Description: S3 path holding the folder zip file, with each folder being a subfolder of the home directory. Each folder will have its own local git repo. To not include any folders, leave blank
    Type: String
    Default: ''

Conditions:
  IsAL2023: !Equals [!Ref InstanceOperatingSystem, 'AmazonLinux-2023']
  IsGraviton: !Not [!Equals [!Select [0, !Split ['g', !Select [0, !Split ['.', !Ref InstanceType]]]],!Select [0, !Split ['.', !Ref InstanceType]]]]

Metadata:
  AWS::CloudFormation::Interface:
    ParameterGroups:
      - Label:
          default: Instance Configuration
        Parameters:
          - InstanceName
          - InstanceVolumeSize
          - InstanceType
          - InstanceOperatingSystem
      - Label:
          default: Code Editor Configuration
        Parameters:
          - CodeEditorUser
          - HomeFolder
          - DevServerBasePath
          - DevServerPort
          - RepoUrl
          - AssetZipS3Path
          - BranchZipS3Path
          - FolderZipS3Path
      - Label:
          default: Kinesis Lab Configuration
        Parameters:
          - EEAssetsBucket
          - EEAssetsKeyPrefix
          - KDADatabaseName
          - KDAKinesisStreamName
    ParameterLabels:
      CodeEditorUser:
        default: Code Editor user name
      InstanceName:
        default: Instance name
      InstanceVolumeSize:
        default: Instance volume size
      InstanceType:
        default: Instance type
      InstanceOperatingSystem:
        default: Instance operating system
      HomeFolder:
        default: Code Editor home folder
      DevServerBasePath:
        default: Application base path
      DevServerPort:
        default: Application port
      RepoUrl:
        default: Git repo URL
      AssetZipS3Path:
        default: Asset file S3 path
      BranchZipS3Path:
        default: Branch file S3 path
      FolderZipS3Path:
        default: Folder file S3 path
      EEAssetsBucket:
        default: Event Engine Assets Bucket
      EEAssetsKeyPrefix:
        default: Event Engine Assets Key Prefix
      KDADatabaseName:
        default: Kinesis Analytics Database Name
      KDAKinesisStreamName:
        default: Kinesis Stream Name

Mappings:
  ArmImage:
    Ubuntu-22:
      ImageId: '{{resolve:ssm:/aws/service/canonical/ubuntu/server/jammy/stable/current/arm64/hvm/ebs-gp2/ami-id}}'
    Ubuntu-24:
      ImageId: '{{resolve:ssm:/aws/service/canonical/ubuntu/server/noble/stable/current/arm64/hvm/ebs-gp3/ami-id}}'
    AmazonLinux-2023:
      ImageId: '{{resolve:ssm:/aws/service/ami-amazon-linux-latest/al2023-ami-kernel-default-arm64}}'
  AmdImage:
    Ubuntu-22:
      ImageId: '{{resolve:ssm:/aws/service/canonical/ubuntu/server/jammy/stable/current/amd64/hvm/ebs-gp2/ami-id}}'
    Ubuntu-24:
      ImageId: '{{resolve:ssm:/aws/service/canonical/ubuntu/server/noble/stable/current/amd64/hvm/ebs-gp3/ami-id}}'
    AmazonLinux-2023:
      ImageId: '{{resolve:ssm:/aws/service/ami-amazon-linux-latest/al2023-ami-kernel-default-x86_64}}'
  AWSRegionsPrefixListID:
    ap-northeast-1:
      PrefixList: pl-58a04531
    ap-northeast-2:
      PrefixList: pl-22a6434b
    ap-south-1:
      PrefixList: pl-9aa247f3
    ap-southeast-1:
      PrefixList: pl-31a34658
    ap-southeast-2:
      PrefixList: pl-b8a742d1
    ca-central-1:
      PrefixList: pl-38a64351
    eu-central-1:
      PrefixList: pl-a3a144ca
    eu-north-1:
      PrefixList: pl-fab65393
    eu-west-1:
      PrefixList: pl-4fa04526
    eu-west-2:
      PrefixList: pl-93a247fa
    eu-west-3:
      PrefixList: pl-75b1541c
    sa-east-1:
      PrefixList: pl-5da64334
    us-east-1:
      PrefixList: pl-3b927c52
    us-east-2:
      PrefixList: pl-b6a144df
    us-west-1:
      PrefixList: pl-4ea04527
    us-west-2:
      PrefixList: pl-82a045eb

Resources:
#S3 Buckets
  TaxiTripDataSet:
    Type: AWS::S3::Bucket
    DeletionPolicy: Retain
    Properties:
      BucketName:
        !Sub
          - 'nyctaxitrips-${AWS::AccountId}-${AWS::Region}-${RandomGUID}'
          - { RandomGUID: !Select [0, !Split ["-", !Select [2, !Split ["/", !Ref AWS::StackId ]]]] }

  CuratedDataSet:
    Type: AWS::S3::Bucket
    DeletionPolicy: Retain
    Properties:
      BucketName:
        !Sub
          - 'curateddata-${AWS::AccountId}-${AWS::Region}-${RandomGUID}'
          - { RandomGUID: !Select [0, !Split ["-", !Select [2, !Split ["/", !Ref AWS::StackId ]]]] }

#Glue database
  Database:
    Type: AWS::Glue::Database
    Properties:
      CatalogId: !Ref 'AWS::AccountId'
      DatabaseInput:
        Name: !Ref KDADatabaseName
        Description: Database for KDA Application Source and Target Tables

#Kinesis Analytics Role
  KinesisAnalyticsRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub 'Kinesis-analytics-KDA-${AWS::StackName}'
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - kinesisanalytics.amazonaws.com
            Action:
              - sts:AssumeRole
      Policies:
        - PolicyName: !Sub 'Kinesis-analytics-KDA-${AWS::StackName}'
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Sid: ReadOnlySid
                Effect: Allow
                Action:
                  - ec2:DescribeVpcs
                  - ec2:DescribeDhcpOptions
                  - ec2:DescribeSubnets
                  - ec2:DescribeSecurityGroups
                Resource:
                  - "*"
              - Sid: LogGroupSid
                Effect: Allow
                Action:
                  - logs:CreateLogGroup
                  - logs:CreateLogStream
                  - logs:PutLogEvents
                  - logs:AssociateKmsKey
                Resource:
                  - arn:aws:logs:*:*:/aws-glue/*
                  - arn:aws:logs:*:*:log-group/aws/kinesis-analytics/*
              - Sid: GlueTableSid
                Effect: Allow
                Action:
                  - glue:GetConnection
                  - glue:GetTable
                  - glue:GetTables
                  - glue:CreateTable
                  - glue:UpdateTable
                  - glue:GetUserDefinedFunction
                  - glue:GetPartitions
                  - glue:DeleteTable
                  - glue:GetDatabase
                  - glue:GetDatabases
                  - glue:GetUserDefinedFunction
                Resource: 
                  - "*"
              - Sid: KinesisEfoConsumer
                Effect: Allow
                Action:
                  - kinesis:DescribeStreamConsumer
                  - kinesis:SubscribeToShard
                Resource:
                  - !Sub 'arn:aws:kinesis:${AWS::Region}:${AWS::AccountId}:stream/${KDAKinesisStreamName}/consumer/*'
                  - !Sub 'arn:aws:kinesis:${AWS::Region}:${AWS::AccountId}:stream/${KDAKinesisStreamName}/consumer/*'
              - Sid: KinesisStreamSid
                Effect: Allow
                Action:
                  - kinesis:GetShardIterator
                  - kinesis:GetRecords
                  - kinesis:PutRecords
                  - kinesis:DescribeStream
                  - kinesis:DescribeStreamSummary
                  - kinesis:RegisterStreamConsumer
                  - kinesis:DeregisterStreamConsumer
                Resource:
                  - !Sub 'arn:aws:kinesis:${AWS::Region}:${AWS::AccountId}:stream/${KDAKinesisStreamName}'
              - Sid: KinesisStreamListShardsID
                Effect: Allow
                Action:
                  - kinesis:*
                Resource:
                  - "*"
              - Sid: S3DataAccessSid
                Effect: Allow
                Action:
                  - s3:*
                Resource:
                  - !Sub 'arn:aws:s3:::${TaxiTripDataSet}'
                  - !Sub 'arn:aws:s3:::${CuratedDataSet}'
                  - !Sub 'arn:aws:s3:::${TaxiTripDataSet}/*'
                  - !Sub 'arn:aws:s3:::${CuratedDataSet}/*'
              - Sid: KinesisAnalyticsSid
                Effect: Allow
                Action:
                  - kinesisanalytics:DescribeApplication
                Resource:
                  - !Sub "arn:aws:kinesisanalytics:${AWS::Region}:${AWS::AccountId}:application/KDA-studio-1-*"
              - Sid: S3AssetsBucket
                Effect: Allow
                Action:
                  - s3:GetObject
                  - s3:GetObjectVersion
                Resource:
                  - !Sub 'arn:aws:s3:::${EEAssetsBucket}/'
                  - !Sub 'arn:aws:s3:::${EEAssetsBucket}/*'

#Kinesis Analytics Studio
  KinesisAnalyticsStudio:
    Type: AWS::KinesisAnalyticsV2::Application
    Properties: 
      ApplicationName: !Sub 'KDA-studio-1-${AWS::StackName}'
      ApplicationDescription: Kinesis Flink Sql Demo
      RuntimeEnvironment: ZEPPELIN-FLINK-2_0
      ApplicationMode: INTERACTIVE
      ServiceExecutionRole: !GetAtt 'KinesisAnalyticsRole.Arn'
      ApplicationConfiguration:
        FlinkApplicationConfiguration:
          ParallelismConfiguration:
            ConfigurationType: CUSTOM
            Parallelism: 4
            ParallelismPerKPU: 1
        ZeppelinApplicationConfiguration:
          CatalogConfiguration:
            GlueDataCatalogConfiguration:
              DatabaseARN: !Sub 'arn:aws:glue:${AWS::Region}:${AWS::AccountId}:database/${Database}'
          CustomArtifactsConfiguration:
            - ArtifactType: DEPENDENCY_JAR
              MavenReference:
                GroupId: org.apache.flink
                ArtifactId: flink-sql-connector-kinesis_2.12
                Version: 1.13.2
            - ArtifactType: DEPENDENCY_JAR
              MavenReference:
                GroupId: org.apache.flink
                ArtifactId: flink-connector-kafka_2.12
                Version: 1.13.2
            - ArtifactType: DEPENDENCY_JAR
              MavenReference:
                GroupId: software.amazon.msk
                ArtifactId: aws-msk-iam-auth
                Version: 1.1.0
            - ArtifactType: DEPENDENCY_JAR
              S3ContentLocation:
                BucketARN: !Sub 'arn:aws:s3:::${EEAssetsBucket}'
                FileKey: !Sub '${EEAssetsKeyPrefix}flink-sql-connector-elasticsearch7_2.11-1.13.2.jar'

  LambdaRole:
    Type: AWS::IAM::Role
    Properties:
      # RoleName: !Sub 'KinesisLambdaConsumerRole-${AWS::StackName}'
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaKinesisExecutionRole
        - arn:aws:iam::aws:policy/service-role/AWSLambdaDynamoDBExecutionRole
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Principal:
            Service:
              - lambda.amazonaws.com
          Action:
            - sts:AssumeRole
      Path: "/"
      Policies:
        - PolicyName: LambdaFunctionPolicy
          PolicyDocument:
           Version: '2012-10-17'
           Statement:
           - Effect: Allow
             Action:
              - logs:CreateLogGroup
             Resource: !Join [ ":" , ["arn:aws:logs",!Ref AWS::Region,!Ref AWS::AccountId,"*" ] ]
           - Effect: Allow
             Action:
              - logs:CreateLogStream
              - logs:PutLogEvents
             Resource: !Join [ ":" , ["arn:aws:logs",!Ref AWS::Region,!Ref AWS::AccountId,"log-group:/aws/lambda/NYCTaxiTrips-DataTransformation-*:*" ] ]
           - Effect: Allow
             Action:
              - dynamodb:PutItem
              - dynamodb:UpdateItem
              - dynamodb:UpdateTable
             Resource: !Join [ ":" , ["arn:aws:dynamodb",!Ref AWS::Region,!Ref AWS::AccountId,"table/kinesisAggs" ] ]
           - Effect: Allow
             Action: 
              - dynamodb:ListContributorInsights
              - dynamodb:ListGlobalTables
              - dynamodb:ListTables
              - dynamodb:ListBackups
              - dynamodb:ListExports
             Resource: "*"
#LambdaFunction
  LambdaFunction:
    Type: AWS::Lambda::Function
    Properties:
      Handler: "index.lambda_handler"
      Role: !GetAtt LambdaRole.Arn
      Runtime: "python3.9"
      Timeout: 10
      FunctionName: !Sub "NYCTaxiTrips-DataTransformation-${AWS::StackName}"
      Code:
        ZipFile: |
          import base64
          import json
          print('Loading function')
          def lambda_handler(event, context):
              output = []
              for record in event['records']:
                  print(record['recordId'])
                  payload = base64.b64decode(record['data']).decode('utf-8')
                  reading =json.loads(payload)
                  # Add additional column source
                  reading['source'] ='NYCTAXI'
                  # Do custom processing on the payload here
                  output_record = {
                      'recordId': record['recordId'],
                      'result': 'Ok',
                      #'data': base64.b64encode(payload.encode('utf-8')
                      'data' : base64.b64encode(json.dumps(reading).encode('UTF-8'))
                  }
                  output.append(output_record)
              print('Successfully processed {} records.'.format(len(event['records'])))
              return {'records': output}

#Custom Lambda Function
  StartKDALambdaFunction:
    Type: AWS::Lambda::Function
    Properties:
      Handler: "index.lambda_handler"
      Role: !GetAtt StartKDALambdaRole.Arn
      Runtime: "python3.9"
      Timeout: 10
      FunctionName: !Sub "StartKDA-${AWS::StackName}"
      Code:
        ZipFile: |
          import os
          import json
          import boto3
          import cfnresponse
          client = boto3.client('kinesisanalyticsv2')     
          def lambda_handler(event, context):
              print(event)
              responseData = {}
              if event['RequestType'] == 'Delete':
                cfnresponse.send(event, context, cfnresponse.SUCCESS, responseData, "CustomResourcePhysicalID")
                return
              application_name = event['ResourceProperties']['ApplicationName']
              try:
                  response = client.start_application(ApplicationName=application_name)
                  print(response)
                  responseValue = "Started the Application"
                  responseData['Data'] = responseValue
              except:
                  responseValue = "Failed Starting the Application, Please start the application manually"
                  responseData['Data'] = responseValue
              cfnresponse.send(event, context, cfnresponse.SUCCESS, responseData, "CustomResourcePhysicalID")

#Custom Lambda Role
  StartKDALambdaRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Principal:
            Service:
              - lambda.amazonaws.com
          Action:
            - sts:AssumeRole
      Path: "/"
      Policies:
        - PolicyName: LambdaFunctionPolicy
          PolicyDocument:
           Version: '2012-10-17'
           Statement:
           - Effect: Allow
             Action:
              - logs:CreateLogGroup
             Resource: !Join [ ":" , ["arn:aws:logs",!Ref AWS::Region,!Ref AWS::AccountId,"*" ] ]
           - Effect: Allow
             Action:
              - logs:CreateLogStream
              - logs:PutLogEvents
             Resource: !Join [ ":" , ["arn:aws:logs",!Ref AWS::Region,!Ref AWS::AccountId,"log-group:/aws/lambda/StartKDA-*:*" ] ]
           - Effect: Allow
             Action:
              - kinesisanalytics:StartApplication
             Resource: !Join [ ":" , ["arn:aws:kinesisanalytics",!Ref AWS::Region,!Ref AWS::AccountId,"application/KDA-studio-*" ] ]

#Start KDA Application
  StartKDA:
    Type: "Custom::StartKDA"
    Properties:
      ServiceToken: !GetAtt StartKDALambdaFunction.Arn
      ApplicationName: !Sub 'KDA-studio-1-${AWS::StackName}'
    DependsOn: KinesisAnalyticsStudio

#Admin password for OpenSearch Instance
  OpenSearchPassword:
    Type: AWS::SecretsManager::Secret
    Properties:
      GenerateSecretString:
        SecretStringTemplate: '{"username": "admin"}'
        GenerateStringKey: password
        PasswordLength: 16
        ExcludeCharacters: "\"@/\\"

#OpenSearch Instance
  OpenSearchInstance:
    Type: AWS::OpenSearchService::Domain
    Properties:
      DomainName: "os-domain"
      EngineVersion: 'OpenSearch_1.2'
      ClusterConfig:
        InstanceCount: 1
        InstanceType: t3.medium.search
      AccessPolicies:
        Version: '2012-10-17'
        Statement:
          - Effect: 'Allow'
            Principal:
              AWS: '*'
            Action: 'es:*'
            Resource: !Sub "arn:aws:es:${AWS::Region}:${AWS::AccountId}:domain/os-domain/*"
      AdvancedSecurityOptions:
        Enabled: true
        InternalUserDatabaseEnabled: true
        MasterUserOptions:
          MasterUserName: !Sub "{{resolve:secretsmanager:${OpenSearchPassword}::username}}"
          MasterUserPassword: !Sub "{{resolve:secretsmanager:${OpenSearchPassword}::password}}"
      NodeToNodeEncryptionOptions:
        Enabled: true
      EncryptionAtRestOptions:
        Enabled: true
      DomainEndpointOptions:
        EnforceHTTPS: true
      EBSOptions:
        EBSEnabled: true
        VolumeSize: 10
        VolumeType: gp2

  # Code Editor Resources
  CodeEditorSecret:
    Metadata:
      cfn_nag:
        rules_to_suppress:
        - id: W77
          reason: The default KMS Key used by Secrets Manager is appropriate for this password which will be used to log into Code Editor, which has very limited permissions. In addition this secret will not be required to be shared across accounts
    Type: AWS::SecretsManager::Secret
    DeletionPolicy: Delete
    UpdateReplacePolicy: Delete
    Properties:
      Name: !Sub
        - ${InstanceName}-${RandomGUID}
        - RandomGUID: !Select [0, !Split ['-', !Select [2, !Split ['/', !Ref AWS::StackId ]]]]
      Description: Code Editor user details
      GenerateSecretString:
        PasswordLength: 16
        SecretStringTemplate: !Sub '{"username":"${CodeEditorUser}"}'
        GenerateStringKey: 'password'
        ExcludePunctuation: true

  SecretPlaintextLambdaRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
        - Effect: Allow
          Principal:
            Service: !Sub lambda.${AWS::URLSuffix}
          Action: sts:AssumeRole
      ManagedPolicyArns:
      - !Sub arn:${AWS::Partition}:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
      - PolicyName: AwsSecretsManager
        PolicyDocument:
          Version: 2012-10-17
          Statement:
          - Effect: Allow
            Action:
            - secretsmanager:GetSecretValue
            Resource: !Ref CodeEditorSecret

  SecretPlaintextLambda:
    Type: AWS::Lambda::Function
    Metadata:
      cfn_nag:
        rules_to_suppress:
        - id: W58
          reason: Warning incorrectly reported. The role associated with the Lambda function has the AWSLambdaBasicExecutionRole managed policy attached, which includes permission to write CloudWatch Logs. See https://docs.aws.amazon.com/aws-managed-policy/latest/reference/AWSLambdaBasicExecutionRole.html
        - id: W89
          reason: CloudFormation custom function does not need the scaffolding of a VPC, to do so would add unnecessary complexity
        - id: W92
          reason: CloudFormation custom function does not need reserved concurrent executions, to do so would add unnecessary complexity
    Properties:
      Description: Return the value of the secret
      Handler: index.lambda_handler
      Runtime: python3.13
      MemorySize: 128
      Timeout: 10
      Architectures:
      - arm64
      Role: !GetAtt SecretPlaintextLambdaRole.Arn
      Code:
        ZipFile: |
          import boto3
          import json
          import cfnresponse
          import logging
          logger = logging.getLogger()
          logger.setLevel(logging.INFO)
          def is_valid_json(json_string):
              logger.debug(f'Calling is_valid_jason:{json_string}')
              try:
                  json.loads(json_string)
                  logger.info(f'Secret is in json format')
                  return True
              except json.JSONDecodeError:
                  logger.info(f'Secret is in string format')
                  return False
          def lambda_handler(event, context):
              logger.debug(f'event: {event}')
              logger.debug(f'context: {context}')
              try:
                  if event['RequestType'] == 'Delete':
                      cfnresponse.send(event, context, cfnresponse.SUCCESS, responseData={}, reason='No action to take')
                  else:
                      resource_properties = event['ResourceProperties']
                      secret_name = resource_properties['SecretArn']
                      secrets_mgr = boto3.client('secretsmanager')
                      logger.info(f'Getting secret from {secret_name}')
                      secret = secrets_mgr.get_secret_value(SecretId = secret_name)
                      logger.debug(f'secret: {secret}')
                      secret_value = secret['SecretString']
                      responseData = {}
                      if is_valid_json(secret_value):
                          responseData = secret_value
                      else:
                          responseData = {'secret': secret_value}
                      logger.debug(f'responseData: {responseData}')
                      logger.debug(f'type(responseData): {type(responseData)}')
                      cfnresponse.send(event, context, cfnresponse.SUCCESS, responseData=json.loads(responseData), reason='OK', noEcho=True)
              except Exception as e:
                  logger.error(e)
                  cfnresponse.send(event, context, cfnresponse.FAILED, responseData={}, reason=str(e))

  SecretPlaintext:
    Type: Custom::SecretPlaintextLambda
    Properties:
      ServiceToken: !GetAtt SecretPlaintextLambda.Arn
      ServiceTimeout: 15
      SecretArn: !Ref CodeEditorSecret

  CodeEditorInstanceBootstrapRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
        - Effect: Allow
          Principal:
            Service:
            - !Sub ec2.${AWS::URLSuffix}
            - !Sub ssm.${AWS::URLSuffix}
          Action: sts:AssumeRole
      ManagedPolicyArns:
      - !Sub arn:${AWS::Partition}:iam::aws:policy/AmazonSSMManagedInstanceCore
      - !Sub arn:${AWS::Partition}:iam::aws:policy/CloudWatchAgentServerPolicy
      - !Sub arn:${AWS::Partition}:iam::aws:policy/AmazonQDeveloperAccess
      - !Sub arn:${AWS::Partition}:iam::aws:policy/ReadOnlyAccess
      - !Sub arn:${AWS::Partition}:iam::aws:policy/AmazonDynamoDBFullAccess
      - !Sub arn:${AWS::Partition}:iam::aws:policy/AmazonKinesisFullAccess
      Policies:
      - PolicyName: KinesisLabAccess
        PolicyDocument:
          Version: 2012-10-17
          Statement:
          - Effect: Allow
            Action:
            - kinesis:*
            - kinesisanalytics:*
            - glue:*
            - s3:GetObject
            - s3:PutObject
            - s3:DeleteObject
            - s3:ListBucket
            - es:*
            - opensearch:*
            - secretsmanager:GetSecretValue
            Resource:
            - !Sub arn:aws:kinesis:${AWS::Region}:${AWS::AccountId}:stream/${KDAKinesisStreamName}
            - !Sub arn:aws:kinesisanalytics:${AWS::Region}:${AWS::AccountId}:application/KDA-studio-1-*
            - !Sub arn:aws:glue:${AWS::Region}:${AWS::AccountId}:database/${Database}
            - !Sub arn:aws:s3:::${TaxiTripDataSet}
            - !Sub arn:aws:s3:::${TaxiTripDataSet}/*
            - !Sub arn:aws:s3:::${CuratedDataSet}
            - !Sub arn:aws:s3:::${CuratedDataSet}/*
            - !Sub arn:aws:es:${AWS::Region}:${AWS::AccountId}:domain/os-domain/*
            - !Ref OpenSearchPassword

  CodeEditorInstanceProfile:
    Type: AWS::IAM::InstanceProfile
    Properties:
      Roles:
      - !Ref CodeEditorInstanceBootstrapRole

  CodeEditorSSMDoc:
    Type: AWS::SSM::Document
    Properties:
      DocumentType: Command
      Content:
        schemaVersion: '2.2'
        description: Bootstrap Code Editor instance
        parameters:
          LinuxFlavor:
            type: String
            default: 'al2023'
          CodeEditorPassword:
            type: String
            default: !Ref AWS::StackId
        mainSteps:
        - name: InstallCloudWatchAgent
          action: aws:configurePackage
          inputs:
            name: AmazonCloudWatchAgent
            action: Install
        - name: ConfigureCloudWatchAgent
          action: aws:runDocument
          inputs:
            documentType: SSMDocument
            documentPath: AmazonCloudWatch-ManageAgent
            documentParameters:
              action: configure
              mode: ec2
              optionalConfigurationSource: default
              optionalRestart: 'yes'
        - name: InstallAptPackagesApt
          action: aws:runShellScript
          precondition:
            StringEquals:
            - '{{ LinuxFlavor }}'
            - ubuntu
          inputs:
            timeoutSeconds: 300
            runCommand:
            - '#!/bin/bash'
            - set -euo pipefail
            - dpkg --configure -a
            - apt-get -q update && DEBIAN_FRONTEND=noninteractive apt-get install -y -q apt-utils
            - apt-get -q update && DEBIAN_FRONTEND=noninteractive apt-get install -y -q needrestart unattended-upgrades
            - sed -i 's/#$nrconf{kernelhints} = -1;/$nrconf{kernelhints} = 0;/' /etc/needrestart/needrestart.conf
            - sed -i 's/#$nrconf{verbosity} = 2;/$nrconf{verbosity} = 0;/' /etc/needrestart/needrestart.conf
            - sed -i "s/#\$nrconf{restart} = 'i';/\$nrconf{restart} = 'a';/" /etc/needrestart/needrestart.conf
            - echo "Apt helper packages added. Checking configuration"
            - cat /etc/needrestart/needrestart.conf
        - name: InstallBasePackagesDnf
          action: aws:runShellScript
          precondition:
            StringEquals:
            - '{{ LinuxFlavor }}'
            - al2023
          inputs:
            timeoutSeconds: 300
            runCommand:
            - '#!/bin/bash'
            - set -euo pipefail
            - dnf install -y --allowerasing curl gnupg whois argon2 unzip nginx openssl jq
        - name: InstallBasePackagesApt
          action: aws:runShellScript
          precondition:
            StringEquals:
            - '{{ LinuxFlavor }}'
            - ubuntu
          inputs:
            timeoutSeconds: 300
            runCommand:
            - '#!/bin/bash'
            - set -euo pipefail
            - dpkg --configure -a
            - apt-get -q update && DEBIAN_FRONTEND=noninteractive apt-get install -y -q curl gnupg whois argon2 unzip nginx openssl locales locales-all apt-transport-https ca-certificates software-properties-common jq
        - name: AddUserDnf
          action: aws:runShellScript
          precondition:
            StringEquals:
            - '{{ LinuxFlavor }}'
            - al2023
          inputs:
            timeoutSeconds: 300
            runCommand:
            - '#!/bin/bash'
            - !Sub |
              echo 'Adding user: ${CodeEditorUser}'
              adduser -c '' ${CodeEditorUser}
              passwd -l ${CodeEditorUser}
              echo "${CodeEditorUser}:{{ CodeEditorPassword }}" | chpasswd
              usermod -aG wheel ${CodeEditorUser}
              sed -i 's/# %wheel/%wheel/g' /etc/sudoers
            - echo "User added. Checking configuration"
            - !Sub getent passwd ${CodeEditorUser}
        - name: AddUserApt
          action: aws:runShellScript
          precondition:
            StringEquals:
            - '{{ LinuxFlavor }}'
            - ubuntu
          inputs:
            timeoutSeconds: 300
            runCommand:
            - '#!/bin/bash'
            - dpkg --configure -a
            - !Sub |
              if [[ "${CodeEditorUser}" == "ubuntu" ]]
              then
                echo 'Using existing user: ${CodeEditorUser}'
              else
                echo 'Adding user: ${CodeEditorUser}'
                adduser --disabled-password --gecos '' ${CodeEditorUser}
                echo "${CodeEditorUser}:{{ CodeEditorPassword }}" | chpasswd
                usermod -aG sudo ${CodeEditorUser}
              fi
            - !Sub |
              tee /etc/sudoers.d/91-vscode-user <<EOF
              ${CodeEditorUser} ALL=(ALL) NOPASSWD:ALL
              EOF
            - !Sub mkdir -p /home/${CodeEditorUser} && chown -R ${CodeEditorUser}:${CodeEditorUser} /home/${CodeEditorUser}
            - !Sub mkdir -p /home/${CodeEditorUser}/.local/bin && chown -R ${CodeEditorUser}:${CodeEditorUser} /home/${CodeEditorUser}
            - echo "User added. Checking configuration"
            - !Sub getent passwd ${CodeEditorUser}
        - name: UpdateProfile
          action: aws:runShellScript
          inputs:
            timeoutSeconds: 300
            runCommand:
            - '#!/bin/bash'
            - echo LANG=en_US.utf-8 >> /etc/environment
            - echo LC_ALL=en_US.UTF-8 >> /etc/environment
            - !Sub echo 'PATH=$PATH:/home/${CodeEditorUser}/.local/bin' >> /home/${CodeEditorUser}/.bashrc
            - !Sub echo 'export PATH' >> /home/${CodeEditorUser}/.bashrc
            - !Sub echo 'export AWS_REGION=${AWS::Region}' >> /home/${CodeEditorUser}/.bashrc
            - !Sub echo 'export AWS_ACCOUNTID=${AWS::AccountId}' >> /home/${CodeEditorUser}/.bashrc
            - !Sub echo 'export NEXT_TELEMETRY_DISABLED=1' >> /home/${CodeEditorUser}/.bashrc
            - !Sub echo "export PS1='\[\033[01;32m\]\u:\[\033[01;34m\]\w\[\033[00m\]\$ '" >> /home/${CodeEditorUser}/.bashrc
            - !Sub chown -R ${CodeEditorUser}:${CodeEditorUser} /home/${CodeEditorUser}
        - name: InstallAWSCLI
          action: aws:runShellScript
          inputs:
            timeoutSeconds: 300
            runCommand:
            - '#!/bin/bash'
            - set -euo pipefail
            - mkdir -p /tmp
            - curl -fsSL https://awscli.amazonaws.com/awscli-exe-linux-$(uname -m).zip -o /tmp/aws-cli.zip
            - !Sub chown -R ${CodeEditorUser}:${CodeEditorUser} /tmp/aws-cli.zip
            - unzip -q -d /tmp /tmp/aws-cli.zip
            - sudo /tmp/aws/install
            - rm -rf /tmp/aws
            - echo "AWS CLI installed. Checking configuration"
            - aws --version
        - name: InstallGitDnf
          action: aws:runShellScript
          precondition:
            StringEquals:
            - '{{ LinuxFlavor }}'
            - al2023
          inputs:
            timeoutSeconds: 300
            runCommand:
            - '#!/bin/bash'
            - set -euo pipefail
            - dnf install -y git
            - !Sub sudo -u ${CodeEditorUser} git config --global user.email "participant@example.com"
            - !Sub sudo -u ${CodeEditorUser} git config --global user.name "Workshop Participant"
            - !Sub sudo -u ${CodeEditorUser} git config --global init.defaultBranch "main"
            - echo "Git installed. Checking configuration"
            - git --version
        - name: InstallGitApt
          action: aws:runShellScript
          precondition:
            StringEquals:
            - '{{ LinuxFlavor }}'
            - ubuntu
          inputs:
            timeoutSeconds: 300
            runCommand:
            - '#!/bin/bash'
            - set -euo pipefail
            - dpkg --configure -a
            - add-apt-repository ppa:git-core/ppa
            - apt-get -q update && DEBIAN_FRONTEND=noninteractive apt-get install -y -q git
            - !Sub sudo -u ${CodeEditorUser} git config --global user.email "participant@example.com"
            - !Sub sudo -u ${CodeEditorUser} git config --global user.name "Workshop Participant"
            - !Sub sudo -u ${CodeEditorUser} git config --global init.defaultBranch "main"
            - echo "Git installed. Checking configuration"
            - git --version
        - name: CloneRepo
          action: aws:runShellScript
          inputs:
            timeoutSeconds: 600
            runCommand:
            - '#!/bin/bash'
            - !Sub |
              if [[ -z "${RepoUrl}" ]]
              then
                echo "No Repo"
              else
                mkdir -p ${HomeFolder} && chown -R ${CodeEditorUser}:${CodeEditorUser} ${HomeFolder}
                sudo -u ${CodeEditorUser} git clone ${RepoUrl} ${HomeFolder}
                echo "Repo ${RepoUrl} cloned. Checking configuration"
                ls -la ${HomeFolder}
                sudo -u ${CodeEditorUser} git -C ${HomeFolder} remote -v
              fi
        - name: DownloadAssets
          action: aws:runShellScript
          inputs:
            timeoutSeconds: 600
            runCommand:
            - '#!/bin/bash'
            - !Sub |
              if [[ -z "${AssetZipS3Path}" ]]
              then
                echo "No assets"
              else
                mkdir -p ${HomeFolder} && chown -R ${CodeEditorUser}:${CodeEditorUser} ${HomeFolder}
                mkdir -p /tmp
                aws s3 cp s3://${AssetZipS3Path} /tmp/asset.zip
                chown -R ${CodeEditorUser}:${CodeEditorUser} /tmp/asset.zip
                unzip -o /tmp/asset.zip -d ${HomeFolder}
                chown -R ${CodeEditorUser}:${CodeEditorUser} ${HomeFolder}
                if  [[ -d ${HomeFolder}/.git ]] # Indicates that a repo has been cloned
                then
                  sudo -u ${CodeEditorUser} git -C ${HomeFolder} add .
                  sudo -u ${CodeEditorUser} git -C ${HomeFolder} commit -m 'Workshop commit'
                else
                  sudo -u ${CodeEditorUser} git -C ${HomeFolder} init
                  sudo -u ${CodeEditorUser} git -C ${HomeFolder} add .
                  sudo -u ${CodeEditorUser} git -C ${HomeFolder} commit -m 'Initial commit'
                fi
                echo "Assets downloaded. Checking configuration: ${HomeFolder}"
                ls -la ${HomeFolder}
                sudo -u ${CodeEditorUser} git -C ${HomeFolder} branch
              fi
        - name: DownloadFolders
          action: aws:runShellScript
          inputs:
            timeoutSeconds: 600
            runCommand:
            - '#!/bin/bash'
            - !Sub |
              if [[ -z "${FolderZipS3Path}" ]]
              then
                echo "No folders"
              else
                rm -rf /tmp/folder
                mkdir -p /tmp/folder && chown -R ${CodeEditorUser}:${CodeEditorUser} /tmp/folder
                aws s3 cp s3://${FolderZipS3Path} /tmp/asset-folder.zip
                chown -R ${CodeEditorUser}:${CodeEditorUser} /tmp/asset-folder.zip
                unzip -o /tmp/asset-folder.zip -d /tmp/folder
                chown -R ${CodeEditorUser}:${CodeEditorUser} /tmp/folder
                mkdir -p ${HomeFolder} && chown -R ${CodeEditorUser}:${CodeEditorUser} ${HomeFolder}
                cd "${HomeFolder}" && cd ..
                if [[ $(pwd) ==  "/" ]]
                then
                  targetRootFolder=""
                else
                  targetRootFolder=$(pwd)
                  chown -R ${CodeEditorUser}:${CodeEditorUser} .
                fi
                find "/tmp/folder" -maxdepth 1 -mindepth 1 -type d | while read sourceFolder; do
                  folder="$(basename $sourceFolder)"
                  echo "Processing folder: $folder"
                  targetFolder=$targetRootFolder/$folder
                  if [[ $targetRootFolder == "" ]]
                  then
                    mv $sourceFolder /
                  else
                    mv $sourceFolder $targetRootFolder
                  fi
                  chown -R ${CodeEditorUser}:${CodeEditorUser} $targetFolder
                  sudo -u ${CodeEditorUser} git -C $targetFolder init
                  sudo -u ${CodeEditorUser} git -C $targetFolder add .
                  sudo -u ${CodeEditorUser} git -C $targetFolder commit -m "Initial commit"
                  echo "Folder downloaded. Checking configuration: $targetFolder"
                  ls -la $targetFolder
                done
                rm -rf /tmp/folder
              fi
        - name: DownloadBranches
          action: aws:runShellScript
          inputs:
            timeoutSeconds: 600
            runCommand:
            - '#!/bin/bash'
            - !Sub |
              if [[ -z "${BranchZipS3Path}" ]]
              then
                echo "No branches"
              else
                rm -rf /tmp/branch
                rm -rf /tmp/git
                mkdir -p /tmp/branch && chown -R ${CodeEditorUser}:${CodeEditorUser} /tmp/branch
                mkdir -p /tmp/git && chown -R ${CodeEditorUser}:${CodeEditorUser} /tmp/git
                aws s3 cp s3://${BranchZipS3Path} /tmp/asset-branch.zip
                chown -R ${CodeEditorUser}:${CodeEditorUser} /tmp/asset-branch.zip
                unzip -o /tmp/asset-branch.zip -d /tmp/branch
                chown -R ${CodeEditorUser}:${CodeEditorUser} /tmp/branch
                mkdir -p ${HomeFolder} && chown -R ${CodeEditorUser}:${CodeEditorUser} ${HomeFolder}
                sudo -u ${CodeEditorUser} git -C ${HomeFolder} init
                mv ${HomeFolder}/.git /tmp/git
                rm -rf ${HomeFolder}
                mkdir -p ${HomeFolder} && chown -R ${CodeEditorUser}:${CodeEditorUser} ${HomeFolder}
                mv /tmp/git/.git ${HomeFolder}
                find /tmp/branch -maxdepth 1 -mindepth 1 -type d | while read sourceFolder; do
                  branch="$(basename $sourceFolder)"
                  echo "Processing branch: $branch"
                  sudo -u ${CodeEditorUser} git -C ${HomeFolder} checkout -b $branch 2>&1
                  cp -a $sourceFolder/. ${HomeFolder}
                  sudo -u ${CodeEditorUser} git -C ${HomeFolder} add .
                  sudo -u ${CodeEditorUser} git -C ${HomeFolder} commit -m "Initial commit $branch"
                  mv ${HomeFolder}/.git /tmp/git
                  rm -rf ${HomeFolder}
                  mkdir ${HomeFolder} && chown -R ${CodeEditorUser}:${CodeEditorUser} ${HomeFolder}
                  mv /tmp/git/.git ${HomeFolder}
                done
                sudo -u ${CodeEditorUser} git -C ${HomeFolder} checkout main 2>&1
                sudo -u ${CodeEditorUser} git -C ${HomeFolder} restore .
                rm -rf /tmp/branch
                rm -rf /tmp/git
                echo "Branches downloaded. Checking configuration: $HomeFolder"
                sudo -u ${CodeEditorUser} git -C ${HomeFolder} branch
                ls -la ${HomeFolder}
              fi
        - name: InstallCodeEditor
          action: aws:runShellScript
          inputs:
            timeoutSeconds: 600
            runCommand:
            - '#!/bin/bash'
            - set -euo pipefail
            - !Sub export CodeEditorUser=${CodeEditorUser}
            - curl -fsSL https://code-editor.amazonaws.com/content/code-editor-server/dist/aws-workshop-studio/install.sh | bash -s -- 2>&1 || { echo "Code Editor installation failed"; exit 1; }
        - name: ConfigureNginx
          action: aws:runShellScript
          inputs:
            timeoutSeconds: 600
            runCommand:
            - '#!/bin/bash'
            - set -euo pipefail
            - echo "Configuring nginx..."
            - !Sub |
              tee /etc/nginx/conf.d/code-editor.conf <<EOF
              server {
                listen 80;
                listen [::]:80;
                server_name *.cloudfront.net;
                proxy_set_header Host \$host;
                proxy_set_header X-Real-IP \$remote_addr;
                proxy_set_header X-Forwarded-For \$proxy_add_x_forwarded_for;
                proxy_set_header X-Forwarded-Proto \$scheme;
                proxy_set_header Upgrade \$http_upgrade;
                proxy_set_header Connection "upgrade";
                proxy_buffering off;
                proxy_set_header Accept-Encoding gzip;
                location / {
                  proxy_pass http://localhost:8080/;
                }
              }
              EOF
            - !Sub |
              if [[ -n "${DevServerBasePath}" ]]
              then
                sed -i '$i\location /${DevServerBasePath} {\proxy_pass http://localhost:${DevServerPort};\}\location /ports/${DevServerPort}/ {\return 301 /${DevServerBasePath};\}' /etc/nginx/conf.d/code-editor.conf
              fi
            - systemctl enable nginx
            - systemctl restart nginx
            - echo "Nginx installed. Checking configuration"
            - nginx -t 2>&1
            - systemctl status nginx
        - name: ConfigureAuthToken
          action: aws:runShellScript
          inputs:
            timeoutSeconds: 600
            runCommand:
            - !Sub |
              sudo -u ${CodeEditorUser} --login mkdir -p /home/${CodeEditorUser}/.code-editor-server/data
              sudo -u ${CodeEditorUser} --login touch /home/${CodeEditorUser}/.code-editor-server/data/token
              echo -n "{{ CodeEditorPassword }}" > /home/${CodeEditorUser}/.code-editor-server/data/token
        - name: CreateCodeEditorSettings
          action: aws:runShellScript
          inputs:
            timeoutSeconds: 600
            runCommand:
            - !Sub sudo -u ${CodeEditorUser} --login mkdir -p /home/${CodeEditorUser}/.code-editor-server/data/User
            - !Sub sudo -u ${CodeEditorUser} --login touch /home/${CodeEditorUser}/.code-editor-server/data/User/settings.json
            - !Sub touch /home/${CodeEditorUser}/.hushlogin
            - !Sub mkdir -p ${HomeFolder} && chown -R ${CodeEditorUser}:${CodeEditorUser} ${HomeFolder}
            - !Sub |
              tee /home/${CodeEditorUser}/.code-editor-server/data/User/settings.json <<EOF
              {
                "aws.telemetry": false,
                "extensions.autoUpdate": false,
                "extensions.autoCheckUpdates": false,
                "telemetry.telemetryLevel": "off",
                "security.workspace.trust.startupPrompt": "never",
                "security.workspace.trust.enabled": false,
                "security.workspace.trust.banner": "never",
                "security.workspace.trust.emptyWindow": false,
                "workbench.startupEditor": "none",
                "terminal.integrated.cwd": "${HomeFolder}",
                "auto-run-command.rules": [
                  {
                    "command": "workbench.action.terminal.new"
                  }
                ]
              }
              EOF
            - !Sub chown -R ${CodeEditorUser}:${CodeEditorUser} /home/${CodeEditorUser}
            - !Sub systemctl enable --now code-editor@${CodeEditorUser}
            - !Sub systemctl restart code-editor@${CodeEditorUser}
        - name: InstallCodeEditorExtensions
          action: aws:runShellScript
          inputs:
            timeoutSeconds: 600
            runCommand:
            - '#!/bin/bash'
            - set -euo pipefail
            - !Sub |
              function install_extension() {
                local extension_name=$1
                echo "Installing extension: $extension_name"
                if ! sudo -u ${CodeEditorUser} --login code-editor-server --install-extension "$extension_name" --force; then
                  echo "Failed to install $extension_name"
                  return 1
                fi
                if sudo -u ${CodeEditorUser} --login code-editor-server --list-extensions | grep -qi "$extension_name"; then
                  echo "Successfully installed $extension_name"
                else
                  echo "Extension $extension_name not found in installed list"
                  return 1
                fi
                return 0
              }
              install_extension "AmazonWebServices.aws-toolkit-vscode" || exit 1
              install_extension "AmazonWebServices.amazon-q-vscode" || exit 1
              install_extension "ms-vscode.live-server" || exit 1
              install_extension "synedra.auto-run-command" || exit 1
            - !Sub chown -R ${CodeEditorUser}:${CodeEditorUser} /home/${CodeEditorUser}
            - echo "Code Editor installed. Checking configuration"
            - !Sub sudo -u ${CodeEditorUser} --login code-editor-server -v
            - !Sub systemctl status code-editor@${CodeEditorUser}

  SSMDocLambdaRole:
    Type: AWS::IAM::Role
    Metadata:
      cfn_nag:
        rules_to_suppress:
        - id: W11
          reason: The Amazon EC2 ssm:*CommandInvocation API actions do not support resource-level permissions, so you cannot control which individual resources users can view in the console. Therefore, the * wildcard is necessary in the Resource element. See https://docs.aws.amazon.com/service-authorization/latest/reference/list_awssystemsmanager.html
    Properties:
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
        - Effect: Allow
          Principal:
            Service: !Sub lambda.${AWS::URLSuffix}
          Action: sts:AssumeRole
      ManagedPolicyArns:
      - !Sub arn:${AWS::Partition}:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
      - PolicyName: SSMDocOnEC2
        PolicyDocument:
          Version: 2012-10-17
          Statement:
          - Effect: Allow
            Action:
            - ssm:SendCommand
            Resource:
            - !Sub arn:${AWS::Partition}:ssm:${AWS::Region}:${AWS::AccountId}:document/${CodeEditorSSMDoc}
            - !Sub arn:${AWS::Partition}:ssm:${AWS::Region}:${AWS::AccountId}:document/AmazonCloudWatch-ManageAgent
            - !Sub arn:${AWS::Partition}:ec2:${AWS::Region}:${AWS::AccountId}:instance/${CodeEditorInstance}
          - Effect: Allow
            Action:
            - ssm:ListCommandInvocations
            - ssm:GetCommandInvocation
            Resource: '*'

  RunSSMDocLambda:
    Type: AWS::Lambda::Function
    Metadata:
      cfn_nag:
        rules_to_suppress:
        - id: W58
          reason: Warning incorrectly reported. The role associated with the Lambda function has the AWSLambdaBasicExecutionRole managed policy attached, which includes permission to write CloudWatch Logs. See https://docs.aws.amazon.com/aws-managed-policy/latest/reference/AWSLambdaBasicExecutionRole.html
        - id: W89
          reason: CloudFormation custom function does not need the scaffolding of a VPC, to do so would add unnecessary complexity
        - id: W92
          reason: CloudFormation custom function does not need reserved concurrent executions, to do so would add unnecessary complexity
    Properties:
      Description: Run SSM document on EC2 instance
      Handler: index.lambda_handler
      Runtime: python3.13
      MemorySize: 128
      Timeout: 600
      Environment:
        Variables:
          RetrySleep: 2900
          AbortTimeRemaining: 3200
      Architectures:
      - arm64
      Role: !GetAtt SSMDocLambdaRole.Arn
      Code:
        ZipFile: |
          import boto3
          import cfnresponse
          import logging
          import time
          import os
          logger = logging.getLogger()
          logger.setLevel(logging.INFO)
          def lambda_handler(event, context):
              logger.debug(f'event: {event}')
              logger.debug(f'context: {context}')
              if event['RequestType'] != 'Create':
                  cfnresponse.send(event, context, cfnresponse.SUCCESS, responseData={}, reason='No action to take')
              else:
                  sleep_ms = int(os.environ.get('RetrySleep'))
                  abort_time_remaining_ms = int(os.environ.get('AbortTimeRemaining'))
                  resource_properties = event['ResourceProperties']
                  instance_id = resource_properties['InstanceId']
                  document_name = resource_properties['DocumentName']
                  cloudwatch_log_group_name = resource_properties['CloudWatchLogGroupName']
                  logger.info(f'Running SSM Document {document_name} on EC2 instance {instance_id}. Logging to {cloudwatch_log_group_name}')
                  del resource_properties['ServiceToken']
                  if 'ServiceTimeout' in resource_properties:
                      del resource_properties['ServiceTimeout']
                  del resource_properties['InstanceId']
                  del resource_properties['DocumentName']
                  del resource_properties['CloudWatchLogGroupName']
                  if 'PhysicalResourceId' in resource_properties:
                      del resource_properties['PhysicalResourceId']
                  logger.debug(f'resource_properties filtered: {resource_properties}')
                  parameters = {}
                  for key, value in resource_properties.items():
                      parameters[key] = [value]
                  logger.debug(f'parameters: {parameters}')
                  retry = True
                  attempt_no = 0
                  time_remaining_ms = context.get_remaining_time_in_millis()
                  ssm = boto3.client('ssm')
                  while (retry == True):
                      attempt_no += 1
                      logger.info(f'Attempt: {attempt_no}. Time Remaining: {time_remaining_ms/1000}s')
                      try:
                          response = ssm.send_command(
                              InstanceIds = [instance_id],
                              DocumentName = document_name,
                              CloudWatchOutputConfig = {'CloudWatchLogGroupName': cloudwatch_log_group_name, 'CloudWatchOutputEnabled': True},
                              Parameters = parameters
                          )
                          logger.debug(f'response: {response}')
                          command_id = response['Command']['CommandId']
                          responseData = {'CommandId': command_id}
                          cfnresponse.send(event, context, cfnresponse.SUCCESS, responseData, reason='OK')
                          retry = False
                      except ssm.exceptions.InvalidInstanceId as e:
                          time_remaining_ms = context.get_remaining_time_in_millis()
                          if (time_remaining_ms > abort_time_remaining_ms):
                              logger.info(f'Instance {instance_id} not ready. Sleeping: {sleep_ms/1000}s')
                              time.sleep(sleep_ms/1000)
                              retry = True
                          else:
                              logger.info(f'Instance {instance_id} not ready, timed out. Time remaining {time_remaining_ms/1000}s < Abort time remaining {abort_time_remaining_ms/1000}s')
                              logger.error(e, exc_info=True)
                              cfnresponse.send(event, context, cfnresponse.FAILED, responseData={}, reason='Timed out. Time remaining: ' + str(time_remaining_ms/1000) + 's < Abort time remaining: ' + str(abort_time_remaining_ms/1000) + 's')
                              retry = False
                      except Exception as e:
                          logger.error(e, exc_info=True)
                          cfnresponse.send(event, context, cfnresponse.FAILED, responseData={}, reason=str(e))
                          retry = False

  RunCodeEditorSSMDoc:
    Type: Custom::RunSSMDocLambda
    Properties:
      ServiceToken: !GetAtt RunSSMDocLambda.Arn
      ServiceTimeout: 305
      InstanceId: !Ref CodeEditorInstance
      DocumentName: !Ref CodeEditorSSMDoc
      CloudWatchLogGroupName: !Sub /aws/ssm/${CodeEditorSSMDoc}
      CodeEditorPassword: !GetAtt SecretPlaintext.password
      LinuxFlavor: !If [IsAL2023, 'al2023', 'ubuntu']

  CodeEditorInstance:
    Type: AWS::EC2::Instance
    Properties:
      ImageId: !If
      - IsGraviton
      - !FindInMap [ArmImage, !Ref InstanceOperatingSystem, ImageId]
      - !FindInMap [AmdImage, !Ref InstanceOperatingSystem, ImageId]
      InstanceType: !Ref InstanceType
      BlockDeviceMappings:
      - DeviceName: !If [IsAL2023, /dev/xvda, /dev/sda1]
        Ebs:
          VolumeSize: !Ref InstanceVolumeSize
          VolumeType: gp3
          DeleteOnTermination: true
          Encrypted: true
      Monitoring: true
      SecurityGroupIds:
      - !Ref SecurityGroup
      IamInstanceProfile: !Ref CodeEditorInstanceProfile
      UserData:
        Fn::Base64: !Sub |
          #cloud-config
          hostname: ${InstanceName}
          runcmd:
          - mkdir -p ${HomeFolder} && chown -R ${CodeEditorUser}:${CodeEditorUser} ${HomeFolder}
      Tags:
      - Key: Name
        Value: !Ref InstanceName

  CodeEditorInstanceCachePolicy:
    Type: AWS::CloudFront::CachePolicy
    Properties:
      CachePolicyConfig:
        DefaultTTL: 86400
        MaxTTL: 31536000
        MinTTL: 1
        Name: !Sub
        - ${InstanceName}-${RandomGUID}
        - RandomGUID: !Select [0, !Split ['-', !Select [2, !Split ['/', !Ref AWS::StackId ]]]]
        ParametersInCacheKeyAndForwardedToOrigin:
          CookiesConfig:
            CookieBehavior: all
          EnableAcceptEncodingGzip: False
          HeadersConfig:
            HeaderBehavior: whitelist
            Headers:
            - Accept-Charset
            - Authorization
            - Origin
            - Accept
            - Referer
            - Host
            - Accept-Language
            - Accept-Encoding
            - Accept-Datetime
          QueryStringsConfig:
            QueryStringBehavior: all

  CloudFrontDistribution:
    Type: AWS::CloudFront::Distribution
    Metadata:
      cfn_nag:
        rules_to_suppress:
        - id: W10
          reason: CloudFront Distribution access logging would require setup of an S3 bucket and changes in IAM, which add unnecessary complexity to the template
        - id: W70
          reason: Workshop Studio does not include a domain that can be used to provision a certificate, so it is not possible to setup TLS. See PFR EE-6016
    Properties:
      DistributionConfig:
        Enabled: True
        HttpVersion: http2and3
        CacheBehaviors:
        - AllowedMethods:
          - GET
          - HEAD
          - OPTIONS
          - PUT
          - PATCH
          - POST
          - DELETE
          CachePolicyId: 4135ea2d-6df8-44a3-9df3-4b5a84be39ad # see https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-managed-cache-policies.html#managed-cache-policy-caching-disabled
          Compress: False
          OriginRequestPolicyId: 216adef6-5c7f-47e4-b989-5492eafa07d3 # Managed-AllViewer - see https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-managed-origin-request-policies.html#:~:text=When%20using%20AWS,47e4%2Db989%2D5492eafa07d3
          TargetOriginId: !Sub CloudFront-${AWS::StackName}
          ViewerProtocolPolicy: allow-all
          PathPattern: '/proxy/*'
        DefaultCacheBehavior:
          AllowedMethods:
          - GET
          - HEAD
          - OPTIONS
          - PUT
          - PATCH
          - POST
          - DELETE
          CachePolicyId: !Ref CodeEditorInstanceCachePolicy
          OriginRequestPolicyId: 216adef6-5c7f-47e4-b989-5492eafa07d3 # Managed-AllViewer - see https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-managed-origin-request-policies.html#:~:text=When%20using%20AWS,47e4%2Db989%2D5492eafa07d3
          TargetOriginId: !Sub CloudFront-${AWS::StackName}
          ViewerProtocolPolicy: allow-all
        Origins:
        - DomainName: !GetAtt CodeEditorInstance.PublicDnsName
          Id: !Sub CloudFront-${AWS::StackName}
          CustomOriginConfig:
            OriginProtocolPolicy: http-only

  SecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Metadata:
      cfn_nag:
        rules_to_suppress:
        - id: F1000
          reason: All outbound traffic should be allowed from this instance. The EC2 instance is provisioned in the default VPC, which already has this egress rule, and it is not possible to duplicate this egress rule in the default VPC
    Properties:
      GroupDescription: Security Group for Code Editor - only allow CloudFront ingress
      SecurityGroupIngress:
      - Description: Allow HTTP from com.amazonaws.global.cloudfront.origin-facing
        IpProtocol: tcp
        FromPort: 80
        ToPort: 80
        SourcePrefixListId: !FindInMap [AWSRegionsPrefixListID, !Ref 'AWS::Region', PrefixList]

  CodeEditorHealthCheckLambdaRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
        - Effect: Allow
          Principal:
            Service: !Sub lambda.${AWS::URLSuffix}
          Action: sts:AssumeRole
      ManagedPolicyArns:
      - !Sub arn:${AWS::Partition}:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole

  CodeEditorHealthCheckLambda:
    Type: AWS::Lambda::Function
    Metadata:
      cfn_nag:
        rules_to_suppress:
        - id: W58
          reason: Warning incorrectly reported. The role associated with the Lambda function has the AWSLambdaBasicExecutionRole managed policy attached, which includes permission to write CloudWatch Logs. See https://docs.aws.amazon.com/aws-managed-policy/latest/reference/AWSLambdaBasicExecutionRole.html
        - id: W89
          reason: CloudFormation custom function does not need the scaffolding of a VPC, to do so would add unnecessary complexity
        - id: W92
          reason: CloudFormation custom function does not need reserved concurrent executions, to do so would add unnecessary complexity
    Properties:
      Description: Run health check on Code Editor instance
      Handler: index.lambda_handler
      Runtime: python3.13
      MemorySize: 128
      Timeout: 600
      Environment:
        Variables:
          RetrySleep: 2900
          AbortTimeRemaining: 5000
      Architectures:
      - arm64
      Role: !GetAtt CodeEditorHealthCheckLambdaRole.Arn
      Code:
        ZipFile: |
          import json
          import cfnresponse
          import logging
          import time
          import os
          import http.client
          from urllib.parse import urlparse
          logger = logging.getLogger()
          logger.setLevel(logging.INFO)
          def healthURLOk(url):
              # Using try block to catch connection errors and JSON conversion errors
              try:
                  logger.debug(f'url: {url}')
                  parsed_url = urlparse(url)
                  if parsed_url.scheme == 'https':
                      logger.debug(f'Trying https: {parsed_url.netloc}. Parsed_url: {parsed_url}')
                      conn = http.client.HTTPSConnection(parsed_url.netloc)
                  else:
                      logger.debug(f'Trying http: {parsed_url.netloc}. Parsed_url: {parsed_url}')
                      conn = http.client.HTTPConnection(parsed_url.netloc)
                  conn.request("GET", parsed_url.path or "/")
                  response = conn.getresponse()
                  logger.debug(f'response: {response}')
                  logger.debug(f'response.status: {response.status}')
                  content = response.read()
                  logger.debug(f'content: {content}')
                  # This will be true for any return code below 4xx (so 3xx and 2xx)
                  if 200 <= response.status < 400:
                      response_dict = json.loads(content.decode('utf-8'))
                      logger.debug(f'response_dict: {response_dict}')
                      # Checking for expected keys and if the key has the expected value
                      if 'hasActiveConnections' in response_dict:
                          # Response code 200 and correct JSON returned
                          logger.info('Health check OK.')
                          return True
                      else:
                          # Response code 200 but the 'hasActiveConnections' key is not present
                          logger.info(f'Health check failed. Response: {response_dict}')
                          return False
                  else:
                      # Response was not ok (error 4xx or 5xx)
                      logger.info(f'Healthcheck failed. Return code: {response.status}')
                      return False
              except http.client.HTTPException as e:
                  # URL malformed or endpoint not ready yet, this should only happen if we can not DNS resolve the URL
                  logger.error(e, exc_info=True)
                  logger.error(f'Healthcheck failed: HTTP Exception. URL invalid and/or endpoint not ready yet')
                  return False
              except json.decoder.JSONDecodeError as e:
                  # The response we got was not a properly formatted JSON
                  logger.error(e, exc_info=True)
                  logger.info(f'Healthcheck failed: Did not get JSON object from URL as expected')
                  return False
              except Exception as e:
                  logger.error(e, exc_info=True)
                  logger.info(f'Healthcheck failed: General error')
                  return False
              finally:
                  if 'conn' in locals():
                      conn.close()
          def lambda_handler(event, context):
              logger.debug(f'event: {event}')
              logger.debug(f'context: {context}')
              try:
                  if event['RequestType'] != 'Create':
                      cfnresponse.send(event, context, cfnresponse.SUCCESS, responseData={}, reason='No action to take')
                  else:
                      sleep_ms = int(os.environ.get('RetrySleep'))
                      abort_time_remaining_ms = int(os.environ.get('AbortTimeRemaining'))
                      resource_properties = event['ResourceProperties']
                      url = resource_properties['Url']
                      logger.info(f'Testing url: {url}')
                      time_remaining_ms = context.get_remaining_time_in_millis()
                      attempt_no = 0
                      health_check = False
                      while (attempt_no == 0 or (time_remaining_ms > abort_time_remaining_ms and not health_check)):
                          attempt_no += 1
                          logger.info(f'Attempt: {attempt_no}. Time Remaining: {time_remaining_ms/1000}s')
                          health_check = healthURLOk(url)
                          if not health_check:
                              logger.debug(f'Healthcheck failed. Sleeping: {sleep_ms/1000}s')
                              time.sleep(sleep_ms/1000)
                              time_remaining_ms = context.get_remaining_time_in_millis()
                      if health_check:
                          logger.info(f'Health check successful. Attempts: {attempt_no}. Time Remaining: {time_remaining_ms/1000}s')
                          cfnresponse.send(event, context, cfnresponse.SUCCESS, responseData={}, reason='Code Editor healthcheck successful')
                      else:
                          logger.info(f'Health check failed. Timed out. Attempts: {attempt_no}. Time remaining {time_remaining_ms/1000}s < Abort time remaining {abort_time_remaining_ms/1000}s')
                          cfnresponse.send(event, context, cfnresponse.FAILED, responseData={}, reason='Code Editor healthcheck failed. Timed out after ' + str(attempt_no) + ' attempts')
                      logger.info(f'Response sent')
              except Exception as e:
                  logger.error(e, exc_info=True)
                  logger.info(f'Health check failed. General exception')
                  cfnresponse.send(event, context, cfnresponse.FAILED, responseData={}, reason=str(e))

  Healthcheck:
    Type: Custom::CodeEditorHealthCheckLambda
    Properties:
      ServiceToken: !GetAtt CodeEditorHealthCheckLambda.Arn
      ServiceTimeout: 610
      Url: !Sub https://${CloudFrontDistribution.DomainName}/healthz

  CheckSSMDocLambda:
    Type: AWS::Lambda::Function
    Metadata:
      cfn_nag:
        rules_to_suppress:
        - id: W58
          reason: Warning incorrectly reported. The role associated with the Lambda function has the AWSLambdaBasicExecutionRole managed policy attached, which includes permission to write CloudWatch Logs. See https://docs.aws.amazon.com/aws-managed-policy/latest/reference/AWSLambdaBasicExecutionRole.html
        - id: W89
          reason: CloudFormation custom function does not need the scaffolding of a VPC, to do so would add unnecessary complexity
        - id: W92
          reason: CloudFormation custom function does not need reserved concurrent executions, to do so would add unnecessary complexity
    Properties:
      Description: Check SSM document on EC2 instance
      Handler: index.lambda_handler
      Runtime: python3.13
      MemorySize: 128
      Timeout: 600
      Environment:
        Variables:
          RetrySleep: 2900
          AbortTimeRemaining: 5000
      Architectures:
      - arm64
      Role: !GetAtt SSMDocLambdaRole.Arn
      Code:
        ZipFile: |
          import boto3
          import cfnresponse
          import logging
          import time
          import os
          logger = logging.getLogger()
          logger.setLevel(logging.INFO)
          def lambda_handler(event, context):
              logger.debug(f'event: {event}')
              logger.debug(f'context: {context}')
              if event['RequestType'] != 'Create':
                  cfnresponse.send(event, context, cfnresponse.SUCCESS, responseData={}, reason='No action to take')
              else:
                  sleep_ms = int(os.environ.get('RetrySleep'))
                  abort_time_remaining_ms = int(os.environ.get('AbortTimeRemaining'))
                  resource_properties = event['ResourceProperties']
                  instance_id = resource_properties['InstanceId']
                  document_name = resource_properties['DocumentName']
                  logger.info(f'Checking SSM Document {document_name} on EC2 instance {instance_id}')
                  retry = True
                  attempt_no = 0
                  time_remaining_ms = context.get_remaining_time_in_millis()
                  ssm = boto3.client('ssm')
                  while (retry == True):
                      attempt_no += 1
                      logger.info(f'Attempt: {attempt_no}. Time Remaining: {time_remaining_ms/1000}s')
                      try:
                          # check to see if document has completed running on instance
                          response = ssm.list_command_invocations(
                              InstanceId=instance_id,
                              Details=True
                          )
                          logger.debug(f'Response: {response}')
                          for invocation in response['CommandInvocations']:
                              if invocation['DocumentName'] == document_name:
                                  invocation_status = invocation['Status']
                                  if invocation_status == 'Success':
                                      logger.info(f'SSM Document {document_name} on EC2 instance {instance_id} complete. Status: {invocation_status}')
                                      cfnresponse.send(event, context, cfnresponse.SUCCESS, responseData={}, reason='OK')
                                      retry = False
                                  elif invocation_status == 'Failed' or invocation_status == 'Cancelled' or invocation_status == 'TimedOut':
                                      logger.info(f'SSM Document {document_name} on EC2 instance {instance_id} failed. Status: {invocation_status}')
                                      reason = ''
                                      # Get information on step that failed, otherwise it's cancelled or timeout
                                      for step in invocation['CommandPlugins']:
                                          step_name = step['Name']
                                          step_status = step['Status']
                                          step_output = step['Output']
                                          logger.debug(f'Step {step_name} {step_status}: {step_output}')
                                          if step_status != 'Success':
                                              try:
                                                  response_step = ssm.get_command_invocation(
                                                      CommandId=invocation['CommandId'],
                                                      InstanceId=instance_id,
                                                      PluginName=step_name
                                                  )
                                                  logger.debug(f'Step details: {response_step}')
                                                  step_output = response_step['StandardErrorContent']
                                              except Exception as e:
                                                  logger.error(e, exc_info=True)
                                              logger.info(f'Step {step_name} {step_status}: {step_output}')
                                              if reason == '':
                                                  reason = f'Step {step_name} {step_status}: {step_output}'
                                              else:
                                                  reason += f'\nStep {step_name} {step_status}: {step_output}'
                                      if reason == '':
                                          reason = f'SSM Document {document_name} on EC2 instance {instance_id} failed. Status: {invocation_status}'
                                      logger.info(f'{reason}')
                                      cfnresponse.send(event, context, cfnresponse.FAILED, responseData={}, reason=reason)
                                      retry = False
                                  else:
                                      logger.info(f'SSM Document {document_name} on EC2 instance {instance_id} not yet complete. Status: {invocation_status}')
                                      retry = True
                          if retry == True:
                              if (time_remaining_ms > abort_time_remaining_ms):
                                  logger.info(f'Sleeping: {sleep_ms/1000}s')
                                  time.sleep(sleep_ms/1000)
                                  time_remaining_ms = context.get_remaining_time_in_millis()
                              else:
                                  logger.info(f'Time remaining {time_remaining_ms/1000}s < Abort time remaining {abort_time_remaining_ms/1000}s')
                                  logger.info(f'Aborting check as time remaining {time_remaining_ms/1000}s < Abort time remaining {abort_time_remaining_ms/1000}s')
                                  cfnresponse.send(event, context, cfnresponse.FAILED, responseData={}, reason='Timed out. Time remaining: ' + str(time_remaining_ms/1000) + 's < Abort time remaining: ' + str(abort_time_remaining_ms/1000) + 's')
                                  retry = False
                      except Exception as e:
                          logger.error(e, exc_info=True)
                          cfnresponse.send(event, context, cfnresponse.FAILED, responseData={}, reason=str(e))
                          retry = False

  CheckCodeEditorSSMDoc:
    Type: Custom::CheckSSMDocLambda
    DependsOn: Healthcheck
    Properties:
      ServiceToken: !GetAtt CheckSSMDocLambda.Arn
      ServiceTimeout: 610
      InstanceId: !Ref CodeEditorInstance
      DocumentName: !Ref CodeEditorSSMDoc

#Outputs will go here
Outputs:
  CodeEditorURL:
    Description: Code Editor URL - Click this link to access your lab environment
    Value: !Sub https://${CloudFrontDistribution.DomainName}/?folder=${HomeFolder}&tkn=${SecretPlaintext.password}
  TaxiTripsS3Bucket:
    Value: !Ref TaxiTripDataSet
    Description: "Taxi Trip Data Set S3 Bucket"
  CuratedS3Bucket:
    Value: !Ref CuratedDataSet
    Description: "Curated Data Set S3 Bucket"
  LambdaFunctionArn:
    Description: Lambda Function ARN
    Value: !GetAtt LambdaFunction.Arn
  GlueDatabaseName:
    Description: Name of the Glue Database
    Value: !Ref Database
  KinesisAnalyticsStudio:
    Description: Kinesis Analytics Studio
    Value: !Ref KinesisAnalyticsStudio
  OpenSearchSecretsManagerReference:
    Description: OpenSearch credentials are stored in secrets manager
    Value: !Select [6, !Split [":", !Ref OpenSearchPassword ]]