{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e89953a",
   "metadata": {},
   "source": [
    "# Laboratorio 1 - Introducción a Spark y RDDs (PySpark)\n",
    "\n",
    "Este notebook introduce los conceptos básicos de Apache Spark usando **RDDs**.\n",
    "\n",
    "En este laboratorio vas a:\n",
    "- Crear/usar un **SparkContext**.\n",
    "- Entender qué es un **RDD** y por qué es *inmutable* y *distribuido*.\n",
    "- Ver la diferencia entre **transformaciones** (lazy) y **acciones**.\n",
    "- Practicar **caché**, **particiones** y operaciones comunes.\n",
    "- Aplicar el patrón *map-reduce* a un caso real: **ventas por tienda**.\n",
    "\n",
    "**Dataset del ejemplo aplicado:** `transacciones_retail_large.csv` (debe estar en el mismo directorio del notebook o ajusta el path).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783e3932",
   "metadata": {},
   "source": [
    "## 1. SparkContext\n",
    "\n",
    "El **SparkContext** (habitualmente `sc`) es el punto de entrada clásico para trabajar con RDDs.\n",
    "En muchos entornos de notebooks `sc` ya existe. Si no existe, lo creamos con `getOrCreate()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dbf9ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Esto le dice a Python: \"Usa el Java que está dentro de este entorno de Anaconda\"\n",
    "os.environ[\"JAVA_HOME\"] = sys.prefix\n",
    "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin;\" + os.environ[\"PATH\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1554e364",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# En notebooks, normalmente puedes reutilizar el contexto existente.\n",
    "sc = SparkContext.getOrCreate()\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29e3f0d",
   "metadata": {},
   "source": [
    "## 2. RDDs (Resilient Distributed Datasets)\n",
    "\n",
    "Un **RDD** es una colección distribuida e inmutable de elementos. Spark registra *cómo* llegar a ese RDD (su *lineage*) y solo ejecuta cuando haces una **acción**.\n",
    "\n",
    "### 2.1 Crear RDDs\n",
    "Una forma rápida es con `parallelize`, útil para ejemplos pequeños.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1def807",
   "metadata": {},
   "outputs": [],
   "source": [
    "intro_rdd = sc.parallelize(range(20), 8)  # 8 particiones\n",
    "print(f\"Particiones: {intro_rdd.getNumPartitions()}\")\n",
    "print(f\"Primeros elementos: {intro_rdd.take(5)}\")\n",
    "intro_rdd.setName(\"RDD de ejemplo\")\n",
    "print(f\"Nombre: {intro_rdd.name()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5a0b2d",
   "metadata": {},
   "source": [
    "### 2.2 Caché\n",
    "\n",
    "El caché es útil cuando vas a reutilizar el mismo RDD en varias acciones. Recuerda que el caché se materializa al ejecutar una acción.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2259dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "intro_rdd.cache()\n",
    "intro_rdd.count()  # acción que materializa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5b6f1d",
   "metadata": {},
   "source": [
    "### 2.3 Particiones\n",
    "\n",
    "Las particiones determinan el paralelismo. Puedes inspeccionarlas con `glom()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809d4ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Número de particiones: {intro_rdd.getNumPartitions()}\")\n",
    "print(f\"Datos por partición: {intro_rdd.glom().collect()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920a2190",
   "metadata": {},
   "source": [
    "### 2.4 Repartition vs Coalesce\n",
    "\n",
    "- `repartition(n)` puede hacer **shuffle completo**.\n",
    "- `coalesce(n)` intenta reducir particiones con menos movimiento de datos (ideal para bajar particiones).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8fdafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "repartitioned = intro_rdd.repartition(6)\n",
    "print(f\"Particiones (repartition): {repartitioned.getNumPartitions()}\")\n",
    "print(repartitioned.glom().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65eba710",
   "metadata": {},
   "outputs": [],
   "source": [
    "coalesced = intro_rdd.coalesce(6)\n",
    "print(f\"Particiones (coalesce): {coalesced.getNumPartitions()}\")\n",
    "print(coalesced.glom().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be920e19",
   "metadata": {},
   "source": [
    "## 3. Transformaciones y acciones\n",
    "\n",
    "Ejemplo rápido:\n",
    "- **Transformaciones**: `filter`, `map`, `flatMap`, `distinct`, etc.\n",
    "- **Acciones**: `count`, `collect`, `take`, `reduce`, `saveAsTextFile`, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a094c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed = (intro_rdd\n",
    "               .filter(lambda x: x % 2 == 0)\n",
    "               .map(lambda x: x + 1))\n",
    "\n",
    "# Nada se ejecuta hasta que hacemos una acción:\n",
    "print(transformed.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4a7402",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_even = (intro_rdd\n",
    "            .filter(lambda x: x % 2 == 0)\n",
    "            .reduce(lambda a, b: a + b))\n",
    "print(sum_even)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa77f82",
   "metadata": {},
   "source": [
    "## 4. Ejemplo aplicado (Init): Ventas totales por tienda con RDD\n",
    "\n",
    "A continuación está el ejemplo base adaptado a tu caso retail (el que empieza por `Init_...`). Calcula `venta_total` por `tienda_id` usando el patrón **map-reduce**.\n",
    "\n",
    "**Columnas esperadas del CSV:**\n",
    "`transaccion_id,timestamp,tienda_id,producto_id,categoria,cantidad,precio_unitario,metodo_pago`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1731bbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basado en Init_RDD_PySpark.py\n",
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "rdd_raw = sc.textFile(\"transacciones_retail_large.csv\")\n",
    "header = rdd_raw.first()\n",
    "rdd_no_header = rdd_raw.filter(lambda linea: linea != header)\n",
    "\n",
    "def parsear_linea(linea):\n",
    "    campos = linea.split(\",\")\n",
    "    return (\n",
    "        campos[2],      # tienda_id\n",
    "        campos[4],      # categoria\n",
    "        int(campos[5]), # cantidad\n",
    "        float(campos[6])# precio_unitario\n",
    "    )\n",
    "\n",
    "rdd_parsed = rdd_no_header.map(parsear_linea)\n",
    "rdd_kv = rdd_parsed.map(lambda x: (x[0], x[2] * x[3]))\n",
    "rdd_totales = rdd_kv.reduceByKey(lambda a, b: a + b)\n",
    "rdd_ordenado = rdd_totales.sortBy(lambda x: x[1], ascending=False)\n",
    "\n",
    "top10 = rdd_ordenado.take(10)\n",
    "print(f\"{'TIENDA':<15} | {'VENTA TOTAL':>15}\")\n",
    "print(\"-\" * 33)\n",
    "for tienda, total in top10:\n",
    "    print(f\"{tienda:<15} | ${total:>14.2f}\")\n",
    "\n",
    "# Nota: evita collect() si el resultado es muy grande."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65739a8c",
   "metadata": {},
   "source": [
    "## 5. Ejercicios sugeridos\n",
    "\n",
    "1. Calcula **venta_total por categoría**.\n",
    "2. Calcula **venta_total por método de pago**.\n",
    "3. Calcula el **ticket promedio por tienda** (venta_total / número de transacciones).\n",
    "4. ¿Qué cambia si en vez de `reduceByKey` usas `groupByKey`? ¿Por qué suele ser peor?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6ecb4b",
   "metadata": {},
   "source": [
    "## 6. Cierre\n",
    "\n",
    "Cuando termines, puedes detener el contexto (opcional en notebooks):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d6a693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
