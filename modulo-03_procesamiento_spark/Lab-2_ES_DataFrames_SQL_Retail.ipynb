{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3e73834",
   "metadata": {},
   "source": [
    "# Laboratorio 2 - DataFrames y Spark SQL (PySpark)\n",
    "\n",
    "En este notebook trabajaremos el mismo caso retail (`transacciones_retail_large.csv`) pero usando:\n",
    "- **DataFrames** (API declarativa, optimizada por Catalyst/Tungsten).\n",
    "- **Spark SQL** (consultas SQL sobre vistas temporales).\n",
    "\n",
    "Objetivo: calcular **ventas totales por tienda** y extender el análisis con consultas adicionales.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5a15de",
   "metadata": {},
   "source": [
    "## 0. Preparación (Init): SparkSession\n",
    "\n",
    "Para DataFrames y Spark SQL, el punto de entrada recomendado es `SparkSession`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4f47e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Retail-DataFrames-SQL\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf0c5aa",
   "metadata": {},
   "source": [
    "## 1. Carga del CSV con esquema\n",
    "\n",
    "Definimos un **schema explícito** para evitar inferencias lentas o errores de tipos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed374c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basado en Init_DataFrame_PySpark.py\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField,\n",
    "    StringType, IntegerType, DoubleType, TimestampType\n",
    ")\n",
    "\n",
    "schema_transacciones = StructType([\n",
    "    StructField(\"transaccion_id\", StringType(), False),\n",
    "    StructField(\"timestamp\", TimestampType(), True),\n",
    "    StructField(\"tienda_id\", StringType(), True),\n",
    "    StructField(\"producto_id\", StringType(), True),\n",
    "    StructField(\"categoria\", StringType(), True),\n",
    "    StructField(\"cantidad\", IntegerType(), True),\n",
    "    StructField(\"precio_unitario\", DoubleType(), True),\n",
    "    StructField(\"metodo_pago\", StringType(), True),\n",
    "])\n",
    "\n",
    "df_ventas = spark.read.csv(\n",
    "    \"transacciones_retail_large.csv\",\n",
    "    header=True,\n",
    "    schema=schema_transacciones\n",
    ")\n",
    "\n",
    "df_ventas.printSchema()\n",
    "df_ventas.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d84fcb",
   "metadata": {},
   "source": [
    "## 2. Transformaciones con DataFrames\n",
    "\n",
    "Calculamos el ingreso por fila (`cantidad * precio_unitario`) y luego agregamos por tienda.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e1a9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, sum as spark_sum, desc\n",
    "\n",
    "df_calculado = df_ventas.withColumn(\n",
    "    \"ingreso_venta\",\n",
    "    col(\"cantidad\") * col(\"precio_unitario\")\n",
    ")\n",
    "\n",
    "df_reporte = (df_calculado\n",
    "              .groupBy(\"tienda_id\")\n",
    "              .agg(spark_sum(\"ingreso_venta\").alias(\"venta_total\"))\n",
    "              .orderBy(desc(\"venta_total\"))\n",
    "             )\n",
    "\n",
    "df_reporte.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0738a845",
   "metadata": {},
   "source": [
    "## 3. El puente a SQL: vista temporal\n",
    "\n",
    "Registramos el DataFrame como una **vista temporal** para consultarla con SQL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fd09d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basado en Init_SparkQL.py\n",
    "df_ventas.createOrReplaceTempView(\"v_transacciones\")\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7a6ee6",
   "metadata": {},
   "source": [
    "## 4. Consulta Spark SQL\n",
    "\n",
    "Escribimos SQL estándar para obtener el total de ventas por tienda.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebf0ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    tienda_id,\n",
    "    SUM(cantidad * precio_unitario) AS venta_total\n",
    "FROM v_transacciones\n",
    "GROUP BY tienda_id\n",
    "ORDER BY venta_total DESC\n",
    "\"\"\"\n",
    "\n",
    "df_resultado_sql = spark.sql(query)\n",
    "df_resultado_sql.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f9a309",
   "metadata": {},
   "source": [
    "## 5. Extensiones útiles (opcional)\n",
    "\n",
    "A continuación, algunas consultas típicas para extender el análisis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34403518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 Ventas por categoría\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "  categoria,\n",
    "  SUM(cantidad * precio_unitario) AS venta_total\n",
    "FROM v_transacciones\n",
    "GROUP BY categoria\n",
    "ORDER BY venta_total DESC\n",
    "\"\"\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5eadc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 Ventas por método de pago\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "  metodo_pago,\n",
    "  SUM(cantidad * precio_unitario) AS venta_total\n",
    "FROM v_transacciones\n",
    "GROUP BY metodo_pago\n",
    "ORDER BY venta_total DESC\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a99bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.3 Ticket promedio por tienda (venta_total / número de transacciones)\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "  tienda_id,\n",
    "  SUM(cantidad * precio_unitario) AS venta_total,\n",
    "  COUNT(*) AS n_transacciones,\n",
    "  (SUM(cantidad * precio_unitario) / COUNT(*)) AS ticket_promedio\n",
    "FROM v_transacciones\n",
    "GROUP BY tienda_id\n",
    "ORDER BY ticket_promedio DESC\n",
    "\"\"\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a5580c",
   "metadata": {},
   "source": [
    "## 6. Ejercicios sugeridos\n",
    "\n",
    "1. Ventas por día (si `timestamp` está completo) usando `DATE(timestamp)`.\n",
    "2. Top 10 productos por venta total.\n",
    "3. ¿Cuál es la distribución de `cantidad` por categoría?\n",
    "4. Compara el plan con `df_resultado_sql.explain(True)` y `df_reporte.explain(True)`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfa465e",
   "metadata": {},
   "source": [
    "## 7. Cierre\n",
    "\n",
    "Cuando termines, puedes detener la sesión:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116fad04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSSpark",
   "language": "python",
   "name": "dsspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
